{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "import zipfile, re, logging\n",
    "from io import BytesIO\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to extract all the nested zip files\n",
    "\n",
    "with zipfile.ZipFile(\"Data.zip\", \"r\") as zfile:\n",
    "    for name in zfile.namelist():\n",
    "        if re.search(r'\\.zip$', name) is not None:\n",
    "            zfiledata = BytesIO(zfile.read(name))\n",
    "            #zfile.extractall()\n",
    "            with zipfile.ZipFile(zfiledata) as zfile2:\n",
    "                zfile2.extractall(\"./Data/\")   \n",
    "                \n",
    "# extracting some information from xml file\n",
    "\n",
    "listText = []\n",
    "listHeadlines = []\n",
    "listFileName = []\n",
    "listItemID = []\n",
    "\n",
    "listTopics = []\n",
    "listTemp = []\n",
    "listPublishedDate = []\n",
    "ListBipTopics = []\n",
    "\n",
    "for filename in os.listdir('./Data/'):\n",
    "    if filename.endswith('.xml'):\n",
    "        with open(os.path.join('./Data/', filename)) as f:\n",
    "            strings = f.read() \n",
    "            f.close()\n",
    "            matchesText = re.findall(r\"(?<=<text>).*?(?=</text>)\", strings, flags=re.DOTALL)\n",
    "            matchesHeadlines = re.findall(r\"(?<=<headline>).*?(?=</headline>)\", strings, flags=re.DOTALL)\n",
    "            matchesItemID = re.findall(r\"<newsitem(?:\\D+=\\\"\\S*\\\")*\\s+itemid=\\\"(\\d*)\\\"\", strings, flags=re.DOTALL)\n",
    "            for text in matchesText:\n",
    "                listText.append(text)\n",
    "            for headline in matchesHeadlines:\n",
    "                listHeadlines.append(headline)\n",
    "            listFileName.append(filename)\n",
    "            for itemid in matchesItemID:\n",
    "                listItemID.append(itemid)    \n",
    "# removing some tags/characetrs from text\n",
    "\n",
    "listText = [txt.replace('<p>', ' ').replace('</p>', ' ').replace('\\n', ' ') for txt in listText]\n",
    "\n",
    "for filename in os.listdir('./Data/'):\n",
    "    if filename.endswith('.xml'):\n",
    "        with open(os.path.join('./Data/', filename)) as f:\n",
    "            strings = f.read() \n",
    "            soup = BeautifulSoup(strings)\n",
    "            listset=soup(\"codes\",\"bip:topics:1.0\")\n",
    "            for top in listset:\n",
    "                listTemp += [a['code'] for a in top.findAll('code',{'code':True})]\n",
    "            listTopics.append(listTemp)\n",
    "            listTemp =[]    \n",
    "            \n",
    "            inputTag = soup(attrs={\"element\" : \"dc.date.published\"})\n",
    "            output = inputTag[0]['value']\n",
    "            listPublishedDate.append(output)\n",
    "\n",
    "for sublist in listTopics:\n",
    "    s = [str(i) for i in sublist]   \n",
    "    res = \",\".join(s) \n",
    "    ListBipTopics.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDataframe(HeadlinesList, TextList, BitopicsList, PublishedDateList, ItemIDList, FileNamesList, columnsList):\n",
    "    df = pd.DataFrame(list(zip(HeadlinesList,TextList,BitopicsList,PublishedDateList,ItemIDList,FileNamesList)), columns=columnsList)\n",
    "    return df\n",
    "\n",
    "def preprocessedData(dataframe, textColumn):\n",
    "    dataframe[textColumn] = dataframe[textColumn].map(lambda x: re.sub(r'\\W+', ' ', x))   #removing special character\n",
    "    dataframe[textColumn] = dataframe[textColumn].map(lambda x: re.sub(r'\\d+', '', x))    # removing all the digits\n",
    "    dataframe[textColumn] = dataframe[textColumn].map(lambda x: x.lower())                # converting into lower case\n",
    "\n",
    "    # tokenize the words\n",
    "    dataframe[textColumn] = dataframe[textColumn].map(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    dataframe[textColumn] = dataframe[textColumn].map(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "    #lemmatization\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    dataframe[textColumn] = dataframe[textColumn].map(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "\n",
    "def featureExtraction(dataframe, textColumn, topicsColumn):\n",
    "    countVect = CountVectorizer(tokenizer=lambda x: x, lowercase=False).fit_transform(dataframe[textColumn])\n",
    "    tfidfTrans = TfidfTransformer()\n",
    "    tfidfOfText = tfidfTrans.fit_transform(countVect)\n",
    "    print(\"Features Shape\",tfidfOfText.shape)\n",
    "    dataframe[topicsColumn]=dataframe[topicsColumn].str.split(',').str[0]\n",
    "    dataframe[topicsColumn] = dataframe[topicsColumn].astype('category')\n",
    "    dataframe[topicsColumn] = dataframe[topicsColumn].cat.codes\n",
    "    return tfidfOfText,dataframe[topicsColumn]\n",
    "\n",
    "def preprocessing(dataframe, textColumn, topicsColumn):\n",
    "    countVect = CountVectorizer(tokenizer=lambda x: x, lowercase=False).fit_transform(dataframe[textColumn])\n",
    "    tfidfTrans = TfidfTransformer()\n",
    "    tfidfOfText = tfidfTrans.fit_transform(countVect)\n",
    "    return tfidfOfText,dataframe[topicsColumn]\n",
    "\n",
    "def splitDataset(Feature, Target, testSize, randomState):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Feature, Target, test_size=testSize, random_state=randomState)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def generateClassifier(features, labels, classifier,gamma='scale',kernel='linear',estimators=200):\n",
    "    labels=labels.astype('int')\n",
    "    X_train, X_test, y_train, y_test = splitDataset(features, labels, 0.3, 25)\n",
    "    if(classifier == DecisionTreeClassifier):\n",
    "        clf = classifier()\n",
    "    elif(classifier == SVC):\n",
    "        clf = classifier(gamma=gamma, kernel=kernel)\n",
    "    elif(classifier == RandomForestClassifier):\n",
    "        clf = classifier(n_estimators=estimators)\n",
    "    elif(classifier == LinearRegression):\n",
    "        clf = classifier()\n",
    "    \n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    return predicted, y_test\n",
    "\n",
    "def evaluateModel(y_test, predictedValues):\n",
    "    accuracyScore = np.mean(predictedValues == y_test)\n",
    "    return accuracyScore\n",
    "# craeting DataFrame and Preprocessing the data    \n",
    "columns = ['HeadLine','Text','Bi:Topics','Date Published','Itemid','XMLfileName']\n",
    "df = extractDataframe(listHeadlines,listText,ListBipTopics,listPublishedDate,listItemID,listFileName,columns)\n",
    "preprocessedData(df,'Text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeadLine</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bi:Topics</th>\n",
       "      <th>Date Published</th>\n",
       "      <th>Itemid</th>\n",
       "      <th>XMLfileName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canadian Occidental mounts rival Wascana bid.</td>\n",
       "      <td>[canadian, occidental, petroleum, ltd, emerged...</td>\n",
       "      <td>C181</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326914</td>\n",
       "      <td>326914newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gruma, Maseca to receive syndicated loan - bank.</td>\n",
       "      <td>[bank, america, launch, three, year, million, ...</td>\n",
       "      <td>C173</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326915</td>\n",
       "      <td>326915newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Too early to call Krupp bid hostile - Deutsche...</td>\n",
       "      <td>[deutsche, bank, ag, management, board, member...</td>\n",
       "      <td>C18,C181,CCAT</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326916</td>\n",
       "      <td>326916newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOCUS - Euro bourses fret over Wall St, electi...</td>\n",
       "      <td>[european, bourse, fell, tuesday, even, wall, ...</td>\n",
       "      <td>M11,M13,M132,M14,M142,MCAT</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326917</td>\n",
       "      <td>326917newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French stocks fall, Alcatel posts big gain.</td>\n",
       "      <td>[french, share, closed, lower, tuesday, second...</td>\n",
       "      <td>G152,M11</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326918</td>\n",
       "      <td>326918newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UK shares end lower, depressed by Dow and poli...</td>\n",
       "      <td>[london, ftse, index, ended, lower, thursday, ...</td>\n",
       "      <td>M11,MCAT</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326919</td>\n",
       "      <td>326919newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FOCUS - Coffee drops after a spectacular rally.</td>\n",
       "      <td>[world, coffee, market, tumble, spectacular, r...</td>\n",
       "      <td>M14,M141,MCAT</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326920</td>\n",
       "      <td>326920newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BOMBAY LEADING STOCKS CLOSING PRICES - March 13.</td>\n",
       "      <td>[today, close, previous, close, bombay, share,...</td>\n",
       "      <td>M11,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368468</td>\n",
       "      <td>368468newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MQM joins Sindh cabinet in Pakistan, governor ...</td>\n",
       "      <td>[pakistani, ethnic, movement, blamed, past, tu...</td>\n",
       "      <td>GCAT,GPOL,GVIO</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368469</td>\n",
       "      <td>368469newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka launches multiple attacks on rebel b...</td>\n",
       "      <td>[sri, lankan, force, launched, air, naval, art...</td>\n",
       "      <td>GCAT,GVIO</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368470</td>\n",
       "      <td>368470newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Indonesian officials safe in helicopter emerge...</td>\n",
       "      <td>[two, indonesian, government, minister, offici...</td>\n",
       "      <td>GCAT,GDIS,GPOL</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368471</td>\n",
       "      <td>368471newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hindu Brahmin convert to succeed Mother Teresa.</td>\n",
       "      <td>[former, hindu, brahmin, converted, roman, cat...</td>\n",
       "      <td>GCAT,GPRO,GREL</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368472</td>\n",
       "      <td>368472newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Indian Shipping -- Visakhapatnam Cargoes - Mar...</td>\n",
       "      <td>[following, berthing, schedule, major, vessel,...</td>\n",
       "      <td>M14,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368473</td>\n",
       "      <td>368473newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bombay stocks end shade lower in moribund trade.</td>\n",
       "      <td>[gmt, share, bombay, stock, exchange, close, s...</td>\n",
       "      <td>M11,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368474</td>\n",
       "      <td>368474newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Indian chemicals prices - Delhi - March 13.</td>\n",
       "      <td>[rate, supplied, asian, news, international, n...</td>\n",
       "      <td>M14,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368475</td>\n",
       "      <td>368475newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Indian oils &amp;amp; oilseeds  prices-Delhi-March...</td>\n",
       "      <td>[rate, supplied, asian, news, international, n...</td>\n",
       "      <td>M14,M141,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368476</td>\n",
       "      <td>368476newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Indian gur &amp;amp; sugar prices - Delhi - March 13.</td>\n",
       "      <td>[rate, supplied, asian, news, international, n...</td>\n",
       "      <td>M14,M141,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368477</td>\n",
       "      <td>368477newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Indian foodgrains prices - Delhi - March 13.</td>\n",
       "      <td>[rate, supplied, asian, news, international, n...</td>\n",
       "      <td>M14,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368478</td>\n",
       "      <td>368478newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>INDIA GOVERNMENT SECURITIES - March 13.</td>\n",
       "      <td>[government, indicative, coupon, yield, year, ...</td>\n",
       "      <td>M12,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368479</td>\n",
       "      <td>368479newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Forthcoming book-closures of Indian companies.</td>\n",
       "      <td>[ace, laboratory, march, march, dividend, aps,...</td>\n",
       "      <td>C15,C151,CCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368480</td>\n",
       "      <td>368480newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Forthcoming board meetings of Indian companies.</td>\n",
       "      <td>[company, date, meeting, purpose, shaw, wallac...</td>\n",
       "      <td>C15,C151,CCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368481</td>\n",
       "      <td>368481newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>INDICATORS - Key Indian economic data-March 13.</td>\n",
       "      <td>[daily, indicator, wednesday, previous, rupee,...</td>\n",
       "      <td>E71,ECAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>368482</td>\n",
       "      <td>368482newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Eurowings/Air France to fly Frankfurt-Berlin.</td>\n",
       "      <td>[german, airline, eurowings, luftverkehrs, ag,...</td>\n",
       "      <td>C11,CCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377564</td>\n",
       "      <td>377564newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Swiss activist claims court win over Scientology.</td>\n",
       "      <td>[swiss, anti, sect, campaigner, claimed, victo...</td>\n",
       "      <td>GCAT,GCRIM,GREL,GVIO</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377565</td>\n",
       "      <td>377565newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Madrid stocks march higher, buoyed by CPI data.</td>\n",
       "      <td>[madrid, stock, bucked, trend, march, higher, ...</td>\n",
       "      <td>M11,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377566</td>\n",
       "      <td>377566newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Alcatel signs $225 mln Indonesia GSM deal.</td>\n",
       "      <td>[france, alcatel, alsthom, said, thursday, sig...</td>\n",
       "      <td>C33,CCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377567</td>\n",
       "      <td>377567newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Dutch bonds close down, but sentiment remains ...</td>\n",
       "      <td>[dutch, bond, ended, thursday, midsession, low...</td>\n",
       "      <td>M12,MCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377568</td>\n",
       "      <td>377568newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Italy's Bulgari to propose stock split.</td>\n",
       "      <td>[italy, bulgari, said, propose, stock, split, ...</td>\n",
       "      <td>C17,C171,CCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377569</td>\n",
       "      <td>377569newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Farm council to vote on cattle, pigs controls.</td>\n",
       "      <td>[farm, minister, asked, vote, proposal, simpli...</td>\n",
       "      <td>C13,CCAT,G15,G153,GCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377570</td>\n",
       "      <td>377570newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Juppe seeks EU VAT cut on CD-ROMS,on-line serv...</td>\n",
       "      <td>[prime, minister, alain, juppe, said, thursday...</td>\n",
       "      <td>C13,CCAT,E21,E211,ECAT,G15,G154,GCAT</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>377571</td>\n",
       "      <td>377571newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48345</th>\n",
       "      <td>PRESS DIGEST - Cyprus - March 31.</td>\n",
       "      <td>[leading, story, greek, cypriot, press, monday...</td>\n",
       "      <td>GCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477856</td>\n",
       "      <td>477856newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48346</th>\n",
       "      <td>PRESS DIGEST - Israel - March 31.</td>\n",
       "      <td>[leading, story, israeli, newspaper, monday, r...</td>\n",
       "      <td>GCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477857</td>\n",
       "      <td>477857newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48347</th>\n",
       "      <td>Iran's Rafsanjani not decided on Saudi haj visit.</td>\n",
       "      <td>[iranian, president, akbar, hashemi, rafsanjan...</td>\n",
       "      <td>GCAT,GDIP,GPOL,GREL</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477858</td>\n",
       "      <td>477858newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48348</th>\n",
       "      <td>FEATURE-Egyptian women suffer violence in sile...</td>\n",
       "      <td>[marlene, tadros, veteran, egyptian, human, ri...</td>\n",
       "      <td>GCAT,GCRIM,GPOL</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477859</td>\n",
       "      <td>477859newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48349</th>\n",
       "      <td>Bank of Portugal injects 4.0 bln esc, 6.5 pct.</td>\n",
       "      <td>[bank, portugal, said, injected, billion, escu...</td>\n",
       "      <td>M13,M131,MCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477860</td>\n",
       "      <td>477860newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48350</th>\n",
       "      <td>Greek 1M ATHIBOR rose to 10.90 from 10.72pct.</td>\n",
       "      <td>[athens, interbank, offered, rate, athibor, on...</td>\n",
       "      <td>M12,M13,M131,MCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477861</td>\n",
       "      <td>477861newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48351</th>\n",
       "      <td>Greek 1M ATHIBOR edges up to 10.72 from 10.72pct.</td>\n",
       "      <td>[athens, interbank, offered, rate, athibor, on...</td>\n",
       "      <td>M12,M13,M131,MCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477862</td>\n",
       "      <td>477862newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48352</th>\n",
       "      <td>PRESS DIGEST - Portugal -- March 31.</td>\n",
       "      <td>[following, leading, domestic, story, portugue...</td>\n",
       "      <td>GCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477863</td>\n",
       "      <td>477863newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48353</th>\n",
       "      <td>PRESS DIGEST - Malta - March 31.</td>\n",
       "      <td>[following, leading, story, maltese, press, mo...</td>\n",
       "      <td>GCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477864</td>\n",
       "      <td>477864newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48354</th>\n",
       "      <td>Bank/Portugal sets 10 bln esc variable rate repo.</td>\n",
       "      <td>[bank, portugal, offered, inject, billion, esc...</td>\n",
       "      <td>M13,M131,MCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477865</td>\n",
       "      <td>477865newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48355</th>\n",
       "      <td>Yield rises on Portuguese 182-day bills.</td>\n",
       "      <td>[average, yield, rose, percent, percent, previ...</td>\n",
       "      <td>M13,M131,MCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477866</td>\n",
       "      <td>477866newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48356</th>\n",
       "      <td>Table of Greek mutual fund flows, assets.</td>\n",
       "      <td>[ionian, bank, reported, following, daily, yea...</td>\n",
       "      <td>C15,C151,CCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477867</td>\n",
       "      <td>477867newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48357</th>\n",
       "      <td>FEATURE - Valencia sets itself ablaze in fiery...</td>\n",
       "      <td>[deafening, blast, thunder, street, burning, c...</td>\n",
       "      <td>GCAT,GENT,GREL</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477868</td>\n",
       "      <td>477868newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48358</th>\n",
       "      <td>FEATURE - Art show recalls happier Franco-Belg...</td>\n",
       "      <td>[quot, shoot, frenchman, quot, go, old, belgia...</td>\n",
       "      <td>GCAT,GDIP,GENT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477869</td>\n",
       "      <td>477869newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48359</th>\n",
       "      <td>NEW YORK, March 31 (Reuter) - Wang Laboratorie.</td>\n",
       "      <td>[wang, laboratory, gave, chairman, chief, exec...</td>\n",
       "      <td>C171</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477870</td>\n",
       "      <td>477870newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48360</th>\n",
       "      <td>Hearst's Ganzi seen in line for CEO post - WSJ.</td>\n",
       "      <td>[hearst, corp, named, victor, ganzi, executive...</td>\n",
       "      <td></td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477871</td>\n",
       "      <td>477871newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48361</th>\n",
       "      <td>Uzbek President Karimov to visit Greece.</td>\n",
       "      <td>[uzbek, president, islam, karimov, leaf, monda...</td>\n",
       "      <td>GDIP</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477872</td>\n",
       "      <td>477872newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48362</th>\n",
       "      <td>St Jude to reduce Ventritex price - Journal.</td>\n",
       "      <td>[st, jude, medical, inc, expected, announce, a...</td>\n",
       "      <td>C31,C33</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477873</td>\n",
       "      <td>477873newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48363</th>\n",
       "      <td>Journal.</td>\n",
       "      <td>[bermuda, based, partnerre, ltd, set, announce...</td>\n",
       "      <td>C33</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477874</td>\n",
       "      <td>477874newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48364</th>\n",
       "      <td>India retains import curbs on consumer electro...</td>\n",
       "      <td>[india, commerce, ministry, said, monday, expo...</td>\n",
       "      <td>E512</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477875</td>\n",
       "      <td>477875newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48365</th>\n",
       "      <td>India rupee recovers, shrugs off political wor...</td>\n",
       "      <td>[indian, rupee, held, steady, dollar, monday, ...</td>\n",
       "      <td>M13,M132,MCAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477876</td>\n",
       "      <td>477876newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48366</th>\n",
       "      <td>Bulgaria seeks advisers for sale of state firms.</td>\n",
       "      <td>[bulgarian, government, seeking, financial, ad...</td>\n",
       "      <td>C183</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477877</td>\n",
       "      <td>477877newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48367</th>\n",
       "      <td>INTERVIEW-Lion Teck Chiang sees net up 70 pct ...</td>\n",
       "      <td>[property, steel, group, lion, teck, chiang, l...</td>\n",
       "      <td>C152</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477878</td>\n",
       "      <td>477878newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48368</th>\n",
       "      <td>World Bank says Bangladesh must open up economy.</td>\n",
       "      <td>[world, bank, asked, bangladesh, reduce, impor...</td>\n",
       "      <td>E11,E12,E51,ECAT</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477879</td>\n",
       "      <td>477879newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48369</th>\n",
       "      <td>Disney Online to charge for kid website - Times.</td>\n",
       "      <td>[walt, disney, co, plan, charge, per, month, a...</td>\n",
       "      <td></td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477880</td>\n",
       "      <td>477880newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48370</th>\n",
       "      <td>U.S. to back fewer supercomputer centers - Times.</td>\n",
       "      <td>[national, science, foundation, plan, reduce, ...</td>\n",
       "      <td></td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477881</td>\n",
       "      <td>477881newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48371</th>\n",
       "      <td>Indian shares plunge 8.6 pct on political crisis.</td>\n",
       "      <td>[indian, share, plunged, eight, percent, panic...</td>\n",
       "      <td>M11</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477882</td>\n",
       "      <td>477882newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48372</th>\n",
       "      <td>Singapore shares open weak, funds stay sidelined.</td>\n",
       "      <td>[singapore, share, opened, weaker, monday, fun...</td>\n",
       "      <td>M11</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477883</td>\n",
       "      <td>477883newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48373</th>\n",
       "      <td>Selecta declares two centavo cash dividend.</td>\n",
       "      <td>[selecta, dairy, product, inc, declared, monda...</td>\n",
       "      <td>C151</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477884</td>\n",
       "      <td>477884newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48374</th>\n",
       "      <td>Tokyo stocks down in thin morning trade.</td>\n",
       "      <td>[tokyo, stock, fell, monday, morning, last, da...</td>\n",
       "      <td>M11</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>477885</td>\n",
       "      <td>477885newsML.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                HeadLine  \\\n",
       "0          Canadian Occidental mounts rival Wascana bid.   \n",
       "1       Gruma, Maseca to receive syndicated loan - bank.   \n",
       "2      Too early to call Krupp bid hostile - Deutsche...   \n",
       "3      FOCUS - Euro bourses fret over Wall St, electi...   \n",
       "4            French stocks fall, Alcatel posts big gain.   \n",
       "...                                                  ...   \n",
       "48370  U.S. to back fewer supercomputer centers - Times.   \n",
       "48371  Indian shares plunge 8.6 pct on political crisis.   \n",
       "48372  Singapore shares open weak, funds stay sidelined.   \n",
       "48373        Selecta declares two centavo cash dividend.   \n",
       "48374           Tokyo stocks down in thin morning trade.   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      [canadian, occidental, petroleum, ltd, emerged...   \n",
       "1      [bank, america, launch, three, year, million, ...   \n",
       "2      [deutsche, bank, ag, management, board, member...   \n",
       "3      [european, bourse, fell, tuesday, even, wall, ...   \n",
       "4      [french, share, closed, lower, tuesday, second...   \n",
       "...                                                  ...   \n",
       "48370  [national, science, foundation, plan, reduce, ...   \n",
       "48371  [indian, share, plunged, eight, percent, panic...   \n",
       "48372  [singapore, share, opened, weaker, monday, fun...   \n",
       "48373  [selecta, dairy, product, inc, declared, monda...   \n",
       "48374  [tokyo, stock, fell, monday, morning, last, da...   \n",
       "\n",
       "                        Bi:Topics Date Published  Itemid       XMLfileName  \n",
       "0                            C181     1997-03-18  326914  326914newsML.xml  \n",
       "1                            C173     1997-03-18  326915  326915newsML.xml  \n",
       "2                   C18,C181,CCAT     1997-03-18  326916  326916newsML.xml  \n",
       "3      M11,M13,M132,M14,M142,MCAT     1997-03-18  326917  326917newsML.xml  \n",
       "4                        G152,M11     1997-03-18  326918  326918newsML.xml  \n",
       "...                           ...            ...     ...               ...  \n",
       "48370                                 1997-03-31  477881  477881newsML.xml  \n",
       "48371                         M11     1997-03-31  477882  477882newsML.xml  \n",
       "48372                         M11     1997-03-31  477883  477883newsML.xml  \n",
       "48373                        C151     1997-03-31  477884  477884newsML.xml  \n",
       "48374                         M11     1997-03-31  477885  477885newsML.xml  \n",
       "\n",
       "[48375 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering the documents\n",
    "\n",
    "### Silhouitte score to find the value of k in k-means clustering algorithm\n",
    "\n",
    "K-means cluster algorithm used to cluster the data. K-means algorithm groups similar data into one cluster, where we need to the the number of cluster k initially. Here I am using elbow method and silhouette coefficient to find the best value for k.\n",
    "\n",
    "As shown in the Silhouette plot, for k = 9, the sum of Silhouette values are the maximum. Hence, I am keeping k=9 for this clustering. \n",
    "\n",
    "I tried DBSCAN, but it didnt work well, DBSCAN groups data into clusters, where some clusters are relatively large and some clusters are relatively small, which is not good for classfication(As some cluster dont have sufficient data to train the model). SO the main goal is here is not just to find the 'accurate' clustering algorithm, but the clustering algorithm, which creates cluster with sufficient data in each of them.\n",
    "\n",
    "Note: we can also throw some clusters away, which very small amount of data, considering that data as outliers. But it is a loss of data, so i preferred clustering the data with sufficient amount of data. Hence, I am using k-means clustering, which gives the efficient results to fulfill these needs.\n",
    "\n",
    "In k-means, we can define the number of cluster and especially for this data, each cluster has enough amount of data so that we can classify each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the value of k for K-Means Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBAAAAHBCAYAAADQP0jdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xu4XHV9L/73B6KActNCNBA0RVEuEWJ2LOhRRBGqgloUSxF/QqGEemzr7VhtPS2kradotRUvPZIUC60tWm2tGChiQdRDvSUBFEWK1lgQvKDcFOXm9/fHTHAn7OyZnezZsy+v1/PkYWbNrLXeM3s9Ieu9v9+1qrUWAAAAgPFsM+wAAAAAwPSnQAAAAAB6UiAAAAAAPSkQAAAAgJ4UCAAAAEBPCgQAAACgJwUCAENRVSdU1SWjnreqenz38blV9WfDSzc1quqkqvp/k7i9Y6rqhqr6cVU9ebK2u8k+Hvg5TSdV9YdV9TfDzpEkVXVGVX1g2DkAYLIpEAAYmKp6elX9R1XdXlU/qqorquopSdJa+4fW2pHDzjhaVV1eVb+1ybJpccI8VrYxvD3J77TWdmytXTlF+5wWWmv/p7U2I7L2o6oWdY+9ecPOAgAb+J8SAANRVTsnWZ3klUn+KclDkzwjyd3DzDXLPTbJV7dkxaratrV2/yTnmRJVNa+1dt+wc0wnvhMABsEIBAAG5QlJ0lo7v7V2f2vtp621S1prX076Gr7/iKq6sKrurKovVNXjNrxQVU+rqi91RzZ8qaqeNuq19VX1nFHPNxpOXlWHdEdF3FZVV1fVYd3lb0mn4HhPdwrAe6rqM93Vru4uO6773qOr6qruNv6jqg7c3Ifo/hb596rqv6rqlqr6i6oa8/+/m/tcY2XbZL3tqurHSbbtZv1md/l+3VEEt1XVV6vqhaPWObeq/m9VXVRVP0nyrE22Od4+n1NV11fVrVX13qqqUeudXFXXdl/7RFU9djOf9eKq+p1Nll1dVS/uPj6rOx3jjqpaW1XPGPW+M6rqI1X1gaq6I8lJY/ycX9j9zLd1v4P9NvmZPH7U8wemzFTVblW1urvej6rqs+P8vA6oqk923/e9qvrDMd5zWFXduMmyB47RqvqVqlrT/Zzfq6q/7L5tw7F3W/f7f2qv77f7uV5VVdcnub46/qqqvt89pr5cVYvH+iwA0A8FAgCD8p9J7q+q86rqeVX1iAmuf3ySFUkekeQbSd6SJFX1yCQXJnlXkl9K8pdJLqyqX+q1waras7vunyV5ZJL/leSfq2r31tqbk3w2v5gC8DuttUO7qx7UXfahqlqa5P1JTuvu/+wkF1TVduPs+pgky5IsTfKiJCePkW2zn2usbKPXba3d3VrbcVTWx1XVQ5J8PMklSeYn+d0k/1BVTxy16svS+V53SvL/NtnmePs8OslTkhyU5NeT/Gr3M/xakj9M8uIku3fXP38z38k/pvMz3vD5909nBMWF3UVfSrIknZ/TPyb5cFVtP2r9FyX5SJJdk/zD6A1X1RO6+31NN8dFST5eVQ/dTJbRXp/kxu56j+p+nrbpm6pqpyT/nuTiJHskeXySS/vY/qbOSnJWa23nJI9LZ7ROkmw49nbtfv+f6/P7/bUkByfZP8mR3e08IZ3v6bgkP9yCjACQRIEAwIC01u5I8vR0Tr5WJflBVV1QVY/qcxP/0lr7YncY9j+kczKZJEclub619vettftaa+cn+XqSF/SxzZcnuai1dlFr7eettU8mWZPk+RP4aKcmObu19oXuyIrz0pmWccg467y1tfaj1tp/J3lnRp04j7I1n2sshyTZMcmZrbV7WmuXpTOlZPS+P9Zau6L7XfxsAts+s7V2W/fzfCq/+NmcluTPW2vXdn9u/yfJks2MQvjoJq+dkM7P/O4kaa19oLX2w+538Y4k2yUZXX58rrX2r93sP91k28clubC19snW2r3pXBtihyRPS2/3JlmQ5LGttXtba59trT2oQEinRPlua+0drbWftdbubK19oY/tj7W/x1fVbq21H7fWPj/Oe/v5fv+8e6z9tLvtnZLsm6S66928BRkBIIkCAYAB6p6wnNRaW5hkcTq/qX1nn6t/d9Tju9I5GU53G9/e5L3fTrJnH9t8bJKXdoen31ZVt6VTcizoM9OGbbx+k23s1c21OTdsknWs927N5xrLHkluaK39fJzt3ZAts7mfzWOTnDXqe/lRksoYn6G1dmc6ow1+o7voNzJqJEFVvb47VP/27rZ2SbJbn9k3+i6738ENY+UYw1+kM+LlkupMO3nTZt63V5Jv9rG9Xk5JZ4TA16szbeXocd7bz/f7wPfSLY3ek+S9Sb5XVSurc20SANgiCgQApkRr7etJzk2nSNgaN6VzIjXaY5J8p/v4J0keNuq1R496fEOSv2+t7Trqz8Nba2duiNnH/m9I8pZNtvGw7oiBzdlrk6w3jfGeXp+rn2ybbm+vTebvj95eP9uc6D5vSHLaJt/NDq21/9jM+89Pcnx3fv8O6YxmSPd6B29MZ3rEI1pruya5PZ2T5X6ybfRddq/RsFd+8dnvymaOke5Igte31vZOZ/TH66rq8M181seNsXxTGx2PVbVtOtMPNuzv+tba8elMM3lrko9U1cM38/n6+X43Wq+19q7W2kiSA9IpKt7QR2YAGJMCAYCBqKp9u79FXth9vlc6w+fHG6Ldj4uSPKGqXlZV86pzYcP90xmenyRXJfmNqnpIVS1LcuyodT+Q5AVV9atVtW1Vbd+9yN3C7uvfS7L3JvvbdNmqJL9dVQd3L1L38Ko6qjsnfnPeUFWP6H4Hr07yoS34XGNlG88X0jl5/f3ud3FYOifEH5zANia6z/cl+YOqOiBJqmqXqnrpOO+/KJ0T/T9J8qFRoyV2SnJfkh8kmVdVf5xkIr85/6ckR1XV4d1rQbw+nWkmG060r0rysu4x8Nwkz9ywYnUukPn4bulwR5L7u382tTrJo6vqNdW5iOVOVXXwGO/7zyTbd4+RhyT53+lMx9iwv5d3r8Hx8yS3dRff3/3sP8/G3/+Evt+qekr3OH1IOsfCzzbzWQCgLwoEAAblznQu5vaF6lzl//NJrknnZG6LtdZ+mM7889enc0G4309ydGvtlu5b/iid3wzfms5FGP9x1Lo3pHPxvT9M5wTthnR+I7vh/4dnJTm2e4X7d3WXnZHkvO6w8V9vra1J5zoI7+nu4xtJTuoR+2NJ1qZz4nphknO24HONlW2zWmv3JHlhkucluSXJXyd5RXckSL8mus+PpvNb9A9W5+4I13T3v7n3353kX5I8J6N+Tkk+keTf0jn5/nY6J759T7dorV2XzvUu3p3OZ39Bkhd0v5OkU+K8IJ0T9hOS/Ouo1fdJ5+KIP07yuSR/3Vq7fIx93JnkiO52vpvk+mxyJ4vu+25P8j+T/E06IyB+ks5FGjd4bpKvVucuGmcl+Y3uNRXuSucCl1d0j71DJvr9plO6rErnOP12OsfV28d5PwCMq8a+LhAAMBmqqiXZp7X2jWFnAQDYGkYgAAAAAD0pEAAAAICeTGEAAAAAejICAQAAAOhJgQAAAAD0pEAAAAAAelIgAAAAAD0pEAAAAICeFAgAAABATwoEAAAAoCcFAgAAANCTAgEAAADoad4gNrrbbru1RYsWDWLTAAAAwFZYu3btLa213Se63kAKhEWLFmXNmjWD2DQAAACwFarq21uynikMc9zKtSuzcu3KYccAAABgmhvICARmjtNWn5YkWT6yfMhJAAAAmM6MQAAAAAB6UiAAAAAAPSkQAAAAgJ4UCAAAAEBPCgQAAACgJwUCAAAA0JPbOM5x7fQ27AgAAADMAEYgAAAAAD0pEAbk5JNPzvz587N48eIHlp1xxhnZc889s2TJkixZsiQXXXTREBMCAABA/xQIA3LSSSfl4osvftDy1772tbnqqqty1VVX5fnPf/4Qkm1sZOVIRlaODDsGAAAA05xrIAzIoYcemvXr1w87Rk/rbl437AgAAADMAEYgTLH3vOc9OfDAA3PyySfn1ltvHXYcAAAA6IsCYQq98pWvzDe/+c1cddVVWbBgQV7/+tcPOxIAAAD0RYEwhR71qEdl2223zTbbbJNTTz01X/ziF4cdCQAAAPriGghT6Oabb86CBQuSJB/96Ec3ukPD1njvb1+25SsvmIRtMCe96n3PHnYEAABgCikQBuT444/P5ZdfnltuuSULFy7MihUrcvnll+eqq65KVWXRokU5++yzhx2TGc5JPAAAMFUUCANy/vnnP2jZKaecMoQk43vaXUcNO8KM5MQdAACYaxQIc9zLbn/dsCNsNSfzAAAAg6dAmAWeffmrxly+39evneIkAAAAzFYKhFloIsXB2pvWJklG9hgZVBwAAABmAQXCLDPRUQfLVi1LkrTT2yDiAAAAMEtsM+wAAAAAwPSnQJhFXPMAAACAQVEgAAAAAD0pEGYJow8AAAAYJAUCAAAA0JMCYRYw+gAAAIBBcxvHOW7NqWuGHQEAAIAZQIEwx43sMTLsCAAAAMwApjDMcNfuu9+wIwAAADAHKBAG5OSTT878+fOzePHiB7329re/PVWVW265ZQjJNrb848uz/OPLhx0DAACAaU6BMCAnnXRSLr744gctv+GGG/LJT34yj3nMY4aQ6sFWrVuVVetWDTsGAAAA05wCYUAOPfTQPPKRj3zQ8te+9rV529velqra6n2YvgAAAMBUUSBMoQsuuCB77rlnDjrooGFHAQAAgAlxF4Ypctddd+Utb3lLLrnkkmFHAQAAgAkzAmGKfPOb38y3vvWtHHTQQVm0aFFuvPHGLF26NN/97ne3aHumLwAAADCVjECYIk960pPy/e9//4HnixYtypo1a7LbbrsNMRUAAAD0xwiEATn++OPz1Kc+Ndddd10WLlyYc845Z9K2PZmjD5YuWJqlC5ZO2vYAAACYnYxAGJDzzz9/3NfXr18/NUF6WLt87bAjAAAAMAMYgTDDuPYBAAAAw6BAAAAAAHpSIMwggxh9UCsqtaImfbsAAADMLgoEAAAAoCcFwgzh2gcAAAAMkwIBAAAA6EmBMAMYfQAAAMCwKRCmOeUBAAAA04ECYRpTHgAAADBdzBt2AMY2VeXB2UefPSX7AQAAYGZTIMxxy0eWDzsCAAAAM4ACYZoxbQEAAIDpyDUQBuTkk0/O/Pnzs3jx4geW/dEf/VEOPPDALFmyJEceeWRuuummjdYZRnmwcu3KrFy7csr3CwAAwMxSrbVJ3+iyZcvamjVrJn27M8lnPvOZ7LjjjnnFK16Ra665Jklyxx13ZOedd06SvOtd78rXvva1vO9979vq4mC/r1+7xevWikqStNMn/zgAAABg+qmqta21ZRNdzxSGATn00EOzfv36jZZtKA+S5Ft/+me57d57c+3ln57iZAAAADBxCoQBu/v66zcaYfDOH/wgF9xxe3bcZpucu9djhpgMAAAA+qdA2EL9TDv4zr33PGjZa3bfPa/Zffes/OEP8w+33Zrf3W33QcQDAACASTUnCoTpeGeDo3beOa+88QYFAgAAADPCnCgQtuYig1tjh/Xrs93RR2e/7kUUr7/++uyzzz5Jkn9/97tz0Kc/nf0+8pGt3s+TznvStNgG0NtXTvzKsCMAAMAWmRMFwjDsvffe+fa3v52f//znWbhwYVasWJG3vvWt+fa3v51tttkmu+yySy677LJhx4RZzwk7AABMDgXCgJx77rkPuo3jXnvtlWc/+9mZN29e3vjGN+a8887LW9/61qHmXLxo8VD3z9zkpB4AAGYeBcKAjHUbxyOPPPKBx4ccckg+MgnTF5ibnIADAABTTYEwJO9///tz3HHHTcq2vvKt/56U7TCDnLHLsBPAxs64fdgJAAAYMAXCELzlLW/JvHnzcsIJJww7Skby4yTJ2uw45CTAwDi5BwBgEigQpth5552X1atX59JLL01VDTtO1tXPOw/acHMAm+HkHwCAaUKBMIUuvvjivPWtb82nP/3pPOxhDxt2HGDQnPwDADCLKBAG5Pjjj8/ll1+eW2655YHbOP75n/957r777hxxxBFJOhdSfN/73jfkpMCDOPEHAIAHUSAMyPnnn/+gZaeccsoQksAs5kQfAACmjAIBmDpO+AEAYMZSIMBc5oQeAADokwJhjju1PWTYEdjAyTwAADCNKRDmuJXZYdgRtp4TbwAAgIFTIMwCi372j8OOsJH1Zx417AgAAABMMgXCHHd3fSNJsl17/BatrywAAACYG7YZdoDZ6uSTT878+fOzePHiB5Z9+MMfzgEHHJBtttkma9asGWK6X/ju9q/Jd7d/zYTWWX/mUQ/8AQAAYG5QIAzISSedlIsvvnijZYsXL86//Mu/5NBDDx1Sqq2jNAAAAJi7TGEYkEMPPTTr16/faNl+++03nDBbSWkAAACAAoHNUhwAAACwgSkMjEl5AAAAwGhGILARxQEAAABjMQKBBygPAAAA2BwFwoAcf/zxeepTn5rrrrsuCxcuzDnnnJOPfvSjWbhwYT73uc/lqKOOyq/+6q8OO2Ye/bN3Zs2pa5QHAAAAjMsUhgE5//zzx1x+zDHHTHGS8d38568edgQAAABmACMQ5jCjDgAAAOiXAmGO2lAeLP/48iz/+PIhpwEAAGC6UyAMyMknn5z58+dn8eLFDyz70Y9+lCOOOCL77LNPjjjiiNx6661Tnmv9mUdtNPJg1bpVWbVu1ZTnAAAAYGZRIAzISSedlIsvvnijZWeeeWYOP/zwXH/99Tn88MNz5plnTmkmUxYAAADYUgqEATn00EPzyEc+cqNlH/vYx3LiiScmSU488cT867/+65Rk2XTUAQAAAEyUuzBMoe9973tZsGBBkmTBggX5/ve/P9D9KQ0AAACYLAqEAXr/+9+fb3zjGznggANy6qmnTsk+lQYAAAAMggJhQK655pp88IMfzN57752rr746z33uc/PIRz4yN998cxYsWJCbb7458+fPn/B2F73pwgctUxoAAAAwaAqEAbn22mvz5Cc/OV/96lczb968PPOZz8y9996b8847L29605ty3nnn5UUvetGEtztWWfDoT121xTnn7bjfVm8DABiO7z5rybAjADCHKBAG5Nxzz80nPvGJVFX23HPPbL/99nnWs56VT37ykznnnHPymMc8Jh/+8IeHHTO/tPQfhx0BYFxOkAAApgcFwoBceOGFOeecc/Le9743O+64Y/bff//ssMMOufTSSyd9X//QXjLp22RyHf7sbw47AgAAwFZRIAzQHXfckbvvvjv33HNPPvOZz+S0004bdqQ5z4k8AADAllEgDMh3vvOd/NVf/VWuu+66/OAHP8jixYszb970+7qf89n/SpL8+zP2HnKSjTnRBwAAmF6m3xntLPK9730vS5YsyXbbbZf9998/++yzz7AjDY1CAAAAYGZTIAzInnvumbe97W1585vfnB122CFHHnlkjjzyyGHHmlRKAQAAgLlDgTAgt956az72sY/lW9/6Vnbddde89KUvzQc+8IG8/OUvH3a0vikIAAAA2ECBMAkuvexxD1r26U//ONtt99N8+SuHJEmeuO+d+acPfyoL9jh90ve/VSf6n62t3wYAAACzngJhEox18r3jw7+Qf/7IyXnqIV/KDjvskL8776Qc8ZxlOfzZvzvp+z/jjDO2fOWahG0wbfg5AgAAg6JAGJCDDz44xx57bJYuXZp58+blyU9+cpYvXz7sWExDTvoBAICZQIEwQCtWrMiKFSuGHWNcR7ejh7ZvJ84AAAAzhwJhFvitnx0+7usLz3zGFCUBAABgttpm2AEYLOUBAAAAk0GBMIv1Ux6sXLsyK9eunII0AAAAzGSmMMxS/Y48OG31aUmS5SMu8AgAAMDmGYEAAAAA9KRAAAAAAHpSIMxCLpwIAADAZFMgAAAAAD0pEAAAAICeFAgAAABAT27jOMtM9PoH7fQ2oCQAAADMJkYgAAAAAD0pEAAAAICeFAhz3MjKkYysHBl2DAAAAKY510CY49bdvG7YEQAAAJgBjEAAAAAAelIgAAAAAD0pEGaRid7CEQAAAPqlQAAAAAB6UiAMyHXXXZclS5Y88GfnnXfOO9/5zmHHAgAAgC3iLgwD8sQnPjFXXXVVkuT+++/PnnvumWOOOWbIqR7s1KWnDjsCAAAAM4ACYQpceumledzjHpfHPvaxw47yICtfsHLYEQAAAJgBTGGYAh/84Adz/PHHDzsGAAAAbDEFwoDdc889ueCCC/LSl750oPvZ0jswrL1pbdbetHaS0wAAADDbmMIwYP/2b/+WpUuX5lGPetSwo4xp2aplSZJ2ehtyEgAAAKYzIxAG7Pzzzzd9AQAAgBlPgTBAd911Vz75yU/mxS9+8bCjAAAAwFYxhWGAHvawh+WHP/zhsGMAAADAVjMCAQAAAOhJgTALbOkdGAAAAKBfCgQAAACgJ9dAmOPWnLpm2BEAAACYARQIc9zIHiPDjgAAAMAMYArDDHfjmz477AgAAADMAQqEOW75x5dn+ceXDzsGAAAA05wCYY5btW5VVq1bNewYAAAATHMKBAAAAKAnBcIM5voHAAAATBUFAgAAANCTAgEAAADoSYEwQ5m+AAAAwFSaN+wADNfSBUuHHQEAAIAZQIEwA03m6IO1y9dO2rYAAACYvUxhAAAAAHpSIMwwrn0AAADAMCgQZpBBlAe1olIratK3CwAAwOyiQAAAAAB6UiDMEKYuAAAAMEwKhBlAeQAAAMCwKRCmOeUBAAAA04ECYRpTHgAAADBdzBt2AB5McQAAAMB007NAqKr/keSq1tpPqurlSZYmOau19u2Bp5tjhlEcnH302VO+TwAAAGaefkYg/N8kB1XVQUl+P8k5Sf4uyTMHGWymmymjCJaPLB92BAAAAGaAfgqE+1prrapelM7Ig3Oq6sRBB5vpFp75jGFHAAAAgEnTz0UU76yqP0jy8iQXVtW2SR4y2Fj06x3HHb1V669cuzIr166cpDQAAADMVv0UCMcluTvJKa217ybZM8lfDDQVU+a01afltNWnDTsGAAAA01zPKQzd0uAvRz3/73SugcCQbe3oAwAAAOjXZguEqrozSRvrpSSttbbzwFIBAAAA08pmC4TW2k5TGQQAAACYvvq5BkKq6ulV9Zvdx7tV1S8PNhYAAAAwnfQsEKrq9CRvTPIH3UUPTfKBQYYCAAAAppd+RiAck+SFSX6SJK21m5KY3jBkLqAIAADAVOp5F4Yk97TWWlW1JKmqhw84E1OonT7WdTIBAABgY/2MQPinqjo7ya5VdWqSf0+yarCxAAAAgOmk5wiE1trbq+qIJHckeUKSP26tfXLgyQAAAIBpo58pDEnylSQ7JGndx8wSIytHkiRrl68dchIAAACms37uwvBbSb6Y5MVJjk3y+ao6edDB2LzJvIDiupvXZd3N6yZtewAAAMxO/YxAeEOSJ7fWfpgkVfVLSf4jyfsHGQwAAACYPvq5iOKNSe4c9fzOJDcMJg4AAAAwHW12BEJVva778DtJvlBVH0vnGggvSmdKAwAAADBHjDeFYafuf7/Z/bPBxwYXBwAAAJiONlsgtNZWTGUQAAAAYPrqeRHFqto9ye8nOSDJ9huWt9aePcBcbMZk3oEhSU5deuqkbg8AAIDZqZ+7MPxDkg8lOTrJbyc5MckPBhmKqbPyBSuHHQEAAIAZoJ+7MPxSa+2cJPe21j7dWjs5ySEDzgUAAABMI/2MQLi3+9+bq+qoJDclWTi4SEyltTetTZKM7DEy5CQAAABMZ/0UCH9WVbskeX2SdyfZOclrB5qKKbNs1bIkSTu9DTkJAAAA01nPAqG1trr78PYkzxpsHAAAAGA62myBUFXvTrLZX0u31n5vIIkAAACAaWe8EQhrpiwFAAAAMK1ttkBorZ03lUHo7R3HHT3sCAAAAMxR/dzGEQAAAJjjFAgAAABAT+PehaGqtk3ye621v5qiPEyxNae61AUAAAC9jVsgtNbur6oXJVEgzFIje4wMOwIAAAAzwLgFQtcVVfWeJB9K8pMNC1tr6waWCgAAAJhW+ikQntb975+MWtaSPHvy4zDVln98eZJk5QtWDjkJAAAA01nPAqG19qypCMJwrFq3KokCAQAAgPH1vAtDVT2qqs6pqn/rPt+/qk4ZfDQAAABguujnNo7nJvlEkj26z/8zyWsGFYixveO4o4cdAQAAgDmsnwJht9baPyX5eZK01u5Lcv9AUwEAAADTSj8Fwk+q6pfSuXBiquqQJLcPNBUAAAAwrfRzF4bXJbkgyeOq6ookuyd56UBTAQAAANNKPwXCV5M8M8kTk1SS69LfyAVmgKULlg47AgAAADNAPwXC51prS9MpEpIkVbUuiTPPWWDt8rXDjgAAAMAMsNkCoaoenWTPJDtU1ZPTGX2QJDsnedgUZAMAAACmifFGIPxqkpOSLEzyjvyiQLgzyR8ONhYAAAAwnWy2QGitnZfkvKp6SWvtn6cwE1OoVnR6oXZ6G3ISAAAAprN+Loa4sKp2ro6/qap1VXXkwJPxgHccd/SwIwAAADDH9VMgnNxauyPJkUnmJ/nNJGcONBUAAAAwrfRTIGy49sHzk/xta+3qUcsAAACAOaCfAmFtVV2SToHwiaraKcnPBxsLAAAAmE76KRBOSfKmJE9prd2V5KHpTGOgh9tuuy3HHnts9t133+y333753Oc+N+xIAAAAsEXGu43jBk/v/vfAKjMXJuLVr351nvvc5+YjH/lI7rnnntx1113DjgQAAABbpJ8C4Q2jHm+f5FeSrE3y7IEkmiXuuOOOfOYzn8m5556bJHnoQx+ahz70ocMNNYazjz572BEAAACYAXoWCK21F4x+XlV7JXnbwBLNEv/1X/+V3XffPb/5m7+Zq6++OiMjIznrrLPy8Ic/fNjRNrJ8ZPmwIwAAADAD9HMNhE3dmGTxZAeZbe67776sW7cur3zlK3PllVfm4Q9/eM48090vAQAAmJl6jkCoqncnad2n2yRZkuTqQYaaDRYuXJiFCxfm4IMPTpIce+yx07JAWLl2ZRIjEQAAABhfP9dAWDPq8X1Jzm+tXTGgPLPGox/96Oy111657rrr8sQnPjGXXnpp9t9//wlv5x3HHT2AdL9w2urjdyLRAAAWrElEQVTTkigQAAAAGF8/10A4byqCzEbvfve7c8IJJ+See+7J3nvvnb/9278ddiQAAADYIpstEKrqK/nF1IWNXkrSWmsHDizVLLFkyZKsWbOm9xsBAABgmhtvBMJgx84DAAAAM8Z4BcJDkjxq0+sdVNUzktw00FQAAADAtDLebRzfmeTOMZb/tPsaAAAAMEeMVyAsaq19edOFrbU1SRYNLBEAAAAw7Yw3hWH7cV7bYbKDMBzt9LGukwkAAAAbG28Ewpeq6tRNF1bVKUnWDi4SAAAAMN2MNwLhNUk+WlUn5BeFwbIkD01yzKCDAQAAANPHZguE1tr3kjytqp6VZHF38YWttcumJBlTYmTlSJJk7XKDSgAAANi88UYgJElaa59K8qkpyMIm3nHc0QPfx7qb1w18HwAAAMx8410DAQAAACCJAgEAAADogwIBAAAA6EmBAAAAAPSkQAAAAAB66nkXBma3U5eeOuwIAAAAzAAKhDlu5QtWDjsCAAAAM4ApDNPUO447etgRAAAA4AEKhDlu7U1rs/amtcOOAQAAwDRnCsMct2zVsiRJO70NOQkAAADTmREIAAAAQE8KhGnI9Q8AAACYbhQIAAAAQE8KBAAAAKAnBQIAAADQkwJhmnH9AwAAAKYjt3Gc49acumbYEQAAAJgBFAhz3MgeI8OOAAAAwAxgCsM0YvoCAAAA05UCYY5b/vHlWf7x5cOOAQAAwDSnQJjjVq1blVXrVg07BgAAANOcAmGaMH0BAACA6UyBAAAAAPSkQAAAAAB6UiBMA6YvAAAAMN0pEAAAAICe5g07AMO1dMHSYUcAAABgBlAgDNmwpy+sXb52qPsHAABgZjCFAQAAAOhJgQAAAAD0pEAYomFPX0iSWlGpFTXsGAAAAExzCgQAAACgJwXCkEyH0QcAAADQLwUCAAAA0JMCYQiMPgAAAGCmUSBMMeUBAAAAM5ECYQopDwAAAJip5g07wFwxXcuDs48+e9gRAAAAmAEUCFNgupYHSbJ8ZPmwIwAAADADKBAGaDoXBwAAADARCoQBmEnFwcq1K5MYiQAAAMD4FAiTaCYVBxuctvq0JAoEAAAAxqdA2EozsTQAAACAiVIg9ElRAAAAwFw2pwsEpQAAAAD0Z1oXCE7wAQAAYHqY1gXC6z+0etgRZoT3/vZlW77ygknYxlZ41fuePZT9AgAAMDHTukBgZlIKAAAAzD4KhDnuPTdfutXbUBgAAADMfgoEtojSAAAAYG5RIAzQokWLstNOO2XbbbfNvHnzsmbNmmFH2mqKAwAAgLlJgTBgn/rUp7LbbrsNO8ZmvXW3306SvPGW9/V8r/IAAABg7lIgzHE3POT6nu9RHAAAALDNsAPMZlWVI488MiMjI1m5cuWw4wAAAMAWMwJhgK644orsscce+f73v58jjjgi++67bw499NBhx+qbkQcAAABsYATCAO2xxx5Jkvnz5+eYY47JF7/4xSEn6p/yAAAAgNEUCAPyk5/8JHfeeecDjy+55JIsXrx4yKn6ozwAAABgU6YwDMj3vve9HHPMMUmS++67Ly972cvy3Oc+d8ipelMeAAAAMBYFwoDsvffeufrqq4cdo6en3XXUA4+VBwAAAGyOAmGOe9ntr0uiPAAAAGB8roGA8gAAAICeFAhz3CF/vEvW3rR22DEAAACY5kxhmMNe9b5np1ZUkqSd3oacBgAAgOnMCIQ5yrQFAAAAJkKBMAcpDwAAAJgoUxjmEMUBAAAAW0qBMGD3339/li1blj333DOrV68eSgbFAQAAAFtLgTBgZ511Vvbbb7/ccccdU75vxQEAAACTRYEwQDfeeGMuvPDCvPnNb85f/uVfDnRfygIAAAAGSYEwQK95zWvytre9LXfeeedA9/Psy1+Va/fdsnU//IhFSZJrz99v8gJBkv2+fu2wIwAAAJNIgTAgq1evzvz58zMyMpLLL7982HE264Bbtx92BKYBJ/sAAEAvCoQBueKKK3LBBRfkoosuys9+9rPccccdefnLX54PfOADw47GLKYIAAAABqVaa5O+0WXLlrU1a9ZM+nZnqssvvzxvf/vbB3YXhmv33fLpB6c/5eYkyYovLZisOEwypQAAADCZqmpta23ZRNczAmGO+/Djb0+iQJhKCgEAAGAmUiBMgcMOOyyHHXbYsGMwQEoBAABgtlMgQBQAAAAAvSgQmHaczAMAAEw/CoRZ4Nf/YMt+jF858SvJikripB0AAIDxKRDmoK+c+JVhRwAAAGCGUSDMMZuWB0sXLB1SEgAAAGYSBcIcMtbIg7XL1w4hCQAAADPNNsMOwNQwbQEAAICtoUCYA5QHAAAAbC0FwizXqzyoFZXq3okBAAAANkeBAAAAAPSkQJjFTF0AAABgsigQAAAAgJ4UCAPys5/9LL/yK7+Sgw46KAcccEBOP/30Kd2/0QcAAABMpnnDDjBbbbfddrnsssuy44475t57783Tn/70PO95z8shhxwy7GgAAAAwYUYgDEhVZccdd0yS3Hvvvbn33ntT5W4HAAAAzExGIAzQ/fffn5GRkXzjG9/Iq171qhx88MFTst+JTF84++izB5gEAACA2UKBMEDbbrttrrrqqtx222055phjcs0112Tx4sXDjrWR5SPLhx0BAACAGcAUhimw66675rDDDsvFF1887CgAAACwRRQIA/KDH/wgt912W5Lkpz/9af793/89++6778D3O9G7L6xcuzIr164cUBoAAABmC1MYBuTmm2/OiSeemPvvvz8///nP8+u//us5+uijhx3rQU5bfVoSUxkAAAAYnwJhQA488MBceeWVw44BAAAAk8IUhllkotMXAAAAoF8KBAAAAKAnBQIAAADQkwJhljB9AQAAgEFSIAAAAAA9uQvDHNdOb8OOAAAAwAxgBMIsYPoCAAAAg6ZAAAAAAHpSIMxxIytHMrJyZNgxAAAAmOYUCDPck8570latv+7mdVl387pJSgMAAMBspUAAAAAAelIgzGBbO/oAAAAA+uU2jjOQ4gAAAICppkCYQRQHAAAADIsCYZpSFgAAADCdKBAyt0/WT1166rAjAAAAMANUa23SN7ps2bK2Zs2aSd8um3HGLkPe/+3D3T8AAAB9q6q1rbVlE13PCAT6pygAAACYsxQIA3LDDTfkFa94Rb773e9mm222yfLly/PqV7962LEeZG3uT5KMZNux36A0AAAAIAqEgZk3b17e8Y53ZOnSpbnzzjszMjKSI444Ivvvv/+wo21kWf0kSdLazhu/oDgAAABgFAXCgCxYsCALFixIkuy0007Zb7/98p3vfGfaFQgPojgAAABgDAqEKbB+/fpceeWVOfjgg4cdZfMUBwAAAIxjm2EHmO1+/OMf5yUveUne+c53Zuedd+69AgAAAExDCoQBuvfee/OSl7wkJ5xwQl784hcPO86DGXUAAABAnxQIA9JayymnnJL99tsvr3vd64Yd58GUBwAAAEyAAmFArrjiivz93/99LrvssixZsiRLlizJRRddNOxYneJAeQAAAMAEuYjigDz96U9Pa23YMTY2RnGw5tQ1QwgCAADATKNAmCs2M+pgZI+RKQ4CAADATKRAmO1MVwAAAGASKBBmqz6Lg+UfX54kWfmClYNMAwAAwAznIoqz0QRGHaxatyqr1q0aYBgAAABmAyMQBuTkk0/O6tWrM3/+/FxzzTWD36GpCgAAAAyQEQgDctJJJ+Xiiy8e/I7O2EV5AAAAwMAZgTAghx56aNavXz+4HZyxy+C2DQAAAJtQIMw0igMAAACGQIEwLIoAAAAAZhAFwrBM4nULFr3pwi1e96HbPW6rtzHbrT/zqGFHAAAAGDoFwhy34O6zhh1hSigBAAAAto4CYUCOP/74XH755bnllluycOHCrFixIqeccsqwY80qSgEAAICpo0AYkPPPP3/YEWYUZQAAAMD0pkCYJbb0BLxWVJKknd4mMw4AAACzzDbDDsDW89t7AAAABk2BAAAAAPSkQAAAAAB6UiAAAAAAPSkQAAAAgJ4UCAAAAEBPbuM4x5199NnDjgAAAMAMoECY45aPLB92BAAAAGYAUxgAAACAnhQIc9zKtSuzcu3KYccAAABgmjOFYY47bfVpSUxlAAAAYHxGIAAAAAA9KRAAAACAnhQIAAAAQE8KBAAAAKAnBQIAAADQkwIBAAAA6Klaa5O/0aofJPn2pG+YzdktyS3DDgGTzHHNbOXYZjZyXDNbObaZrZ7YWttpoivNG0SS1trug9guY6uqNa21ZcPOAZPJcc1s5dhmNnJcM1s5tpmtqmrNlqxnCgMAAADQkwIBAAAA6EmBMDusHHYAGADHNbOVY5vZyHHNbOXYZrbaomN7IBdRBAAAAGYXIxAAAACAnhQIM0RVPbeqrquqb1TVm8Z4fbuq+lD39S9U1aKpTwkT18ex/bqq+lpVfbmqLq2qxw4jJ0xEr+N61PuOrapWVa7wzYzQz7FdVb/e/Xv7q1X1j1OdEbZEH/8eeUxVfaqqruz+m+T5w8gJE1FV76+q71fVNZt5varqXd3j/stVtbTXNhUIM0BVbZvkvUmel2T/JMdX1f6bvO2UJLe21h6f5K+SvHVqU8LE9XlsX5lkWWvtwCQfSfK2qU0JE9PncZ2q2inJ7yX5wtQmhC3Tz7FdVfsk+YMk/6O1dkCS10x5UJigPv/e/t9J/qm19uQkv5Hkr6c2JWyRc5M8d5zXn5dkn+6f5Un+b68NKhBmhl9J8o3W2n+11u5J8sEkL9rkPS9Kcl738UeSHF5VNYUZYUv0PLZba59qrd3Vffr5JAunOCNMVD9/ZyfJn6ZTiP1sKsPBVujn2D41yXtba7cmSWvt+1OcEbZEP8d2S7Jz9/EuSW6awnywRVprn0nyo3He8qIkf9c6Pp9k16paMN42FQgzw55Jbhj1/MbusjHf01q7L8ntSX5pStLBluvn2B7tlCT/NtBEsPV6HtdV9eQke7XWVk9lMNhK/fyd/YQkT6iqK6rq81U13m++YLro59g+I8nLq+rGJBcl+d2piQYDNdF/i2feQOMwWcYaSbDp7TP6eQ9MN30ft1X18iTLkjxzoIlg6417XFfVNulMNTtpqgLBJOnn7+x56QyFPSydEWOfrarFrbXbBpwNtkY/x/bxSc5trb2jqp6a5O+7x/bPBx8PBmbC55BGIMwMNybZa9TzhXnwsKkH3lNV89IZWjXecBWYDvo5tlNVz0ny5iQvbK3dPUXZYEv1Oq53SrI4yeVVtT7JIUkucCFFZoB+/z3ysdbava21byW5Lp1CAaazfo7tU5L8U5K01j6XZPsku01JOhicvv4tPpoCYWb4UpJ9quqXq+qh6Vy45YJN3nNBkhO7j49NcllrzQgEpruex3Z3qPfZ6ZQH5tIyE4x7XLfWbm+t7dZaW9RaW5TOtT1e2FpbM5y40Ld+/j3yr0melSRVtVs6Uxr+a0pTwsT1c2z/d5LDk6Sq9kunQPjBlKaEyXdBkld078ZwSJLbW2s3j7eCKQwzQGvtvqr6nSSfSLJtkve31r5aVX+SZE1r7YIk56QzlOob6Yw8+I3hJYb+9Hls/0WSHZN8uHtd0P9urb1waKGhhz6Pa5hx+jy2P5HkyKr6WpL7k7yhtfbD4aWG3vo8tl+fZFVVvTadId4n+WUd011VnZ/OlLLdutfvOD3JQ5Kktfa+dK7n8fwk30hyV5Lf7LlNxz0AAADQiykMAAAAQE8KBAAAAKAnBQIAAADQkwIBAAAA6EmBAAAAAPSkQACArqp6c1V9taq+XFVXVdXB3eV/U1X7dx+vr6rdqmpRVV0z4DyLquplo54vqarnD3Kf42TZvaq+UFVXVtUzquqlVXVtVX2qqpZV1bt6rH9RVe26hfv+tQ3f/9aqqjOq6n9NxrYAYK6ZN+wAADAdVNVTkxydZGlr7e6q2i3JQ5OktfZbQ4q1KMnLkvxj9/mSJMvSuW/zVDs8yddbaycmSVVdnOR/ttY+1X19zXgrt9a2pvj4tSSrk3xtK7YBAGwlIxAAoGNBkltaa3cnSWvtltbaTUlSVZdX1bIx1tm2qlZ1Ry1cUlU7dN+/pKo+3x3J8NGqesSm2+mOYljffbxtVf1FVX2pu85p3e2fmeQZ3dEQb0zyJ0mO6z4/rqoeXlXv7653ZVW9aKwPVlW/X1Vfqaqrq+rMHhkfV1UXV9XaqvpsVe1bVUuSvC3J87v7Pj3J05O8r5v7sKpa3V1/x6r62+7+vlxVL+kuX98tZVJVL6+qL3a3dXZVbdtd/uOqeks35+er6lFV9bQkL0zyF933P27U59qlu91tus8fVlU3VNVDqurU7vdydVX9c1U9bIzvZUI/j6paUFWf6ea4pqqesfnDCQBmHwUCAHRckmSvqvrPqvrrqnpmH+vsk+S9rbUDktyW5CXd5X+X5I2ttQOTfCXJ6T22c0qS21trT0nylCSnVtUvJ3lTks+21pa01t6a5I+TfKj7/ENJ3pzksu56z0rnJPvhozdcVc9L5zf4B7fWDkqnCBgv48okv9taG0nyv5L8dWvtqk32vSKdEQcntNbesMln+aPuZ3lSd9uXbZJnvyTHJfkfrbUlSe5PckL35Ycn+Xw352eSnNpa+48kFyR5Q3ff39ywrdba7UmuTrLhZ/WCJJ9ord2b5F9aa0/pbuva7nfcr839PF7W3f6SJAcluWoC2wSAGc8UBgBI0lr7cVWNJHlGOifjH6qqN7XWzh1ntW91T66TZG2SRVW1S5JdW2uf7i4/L8mHe+z+yCQHVtWx3ee7pFNO3NPHei8cNad/+ySPSeeEeYPnJPnb1tpd3c/5o81lrKodkzyt+3jD+tv1yLCp5yT5jQ1PWmu3bvL64UlGknypu48dkny/+9o96UxVSDrf5xF97O9D6RQSn+ru96+7yxdX1Z8l2TXJjkk+MYHPsLmfx5eSvL+qHpLkX0f97AFgTlAgAEBXa+3+JJcnubyqvpLkxCTnjrPK3aMe35/OyfB47ssvRv9tP2p5pfNb/41OcqvqsB7bqyQvaa1d1+M9rcd2NtgmyW3d37BvqV77qyTntdb+YIzX7m2tbVj3/vT375QLkvx5VT0ynWJiw4iHc5P8Wmvt6qo6KclhY6w7oZ9HklTVoUmOSvL3VfUXrbW/6yMjAMwKpjAAQJKqemJV7TNq0ZIk357odrrD6m8dNT/+/0uy4Tf969M5yU2SY0et9okkr+z+ZjtV9YTuVIQ7k+w06n2bPv9Ekt+t7q/yq+rJY0S6JMnJG64BUFWP3FzG1todSb5VVS/tvreq6qAJfQGd/f3Ohicbrq0wyqVJjq2q+RvyVNVje2xz08/9gNbaj5N8MclZSVZ3S6B0339z9zs9Yax1M8GfRzfn91trq5Kck2Rpj9wAMKsoEACgY8ck51XV16rqy0n2T3LGFm7rxHSuR/DldIqIP+kuf3s6J6b/kWS3Ue//m3TuMLCuOreGPDud375/Ocl93QsBvjadYfr7dy/id1ySP03ykCRf7q73p5sGaa1dnM5v6ddU1VXpXNdgvIwnJDmlqq5O8tUkY16YcRx/luQR3YsMXp3OdJDReb6W5H8nuaS770+mcwHL8XwwyRuqc6HIx43x+oeSvLz73w3+KMkXutv/+ma2O9Gfx2FJrqqqK9O53sVZPXIDwKxSvxgpCAAAADA2IxAAAACAnhQIAAAAQE8KBAAAAKAnBQIAAP9/O3YgAAAAACDI33qEBQojAFgCAQAAAFgCAQAAAFgCAQAAAFgCAQAAAFgBMvr7VBO3x10AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1296x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, k in enumerate([15,20]):\n",
    "    fig, (ax1) = plt.subplots(1)\n",
    "    fig.set_size_inches(15, 5)\n",
    "    \n",
    "    # Kmeans algorithm\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    labels = kmeans.fit_predict(Features)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    silhouetteSamples = silhouette_samples(Features, labels)\n",
    "\n",
    "    # Silhouette graph\n",
    "    yLlower, yUpper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        vals = silhouetteSamples[labels == cluster]\n",
    "        silhouetteSamples.sort()\n",
    "        yUpper += len(vals)\n",
    "        ax1.barh(range(yLlower, yUpper), vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (yLlower + yUpper) / 2, str(i + 1))\n",
    "        yLlower += len(vals)\n",
    "\n",
    "    ave = np.mean(silhouetteSamples)\n",
    "    ax1.axvline(ave, linestyle='--', color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette coefficient values for different clusters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, k in enumerate([5,9]):\n",
    "    fig, (ax1) = plt.subplots(1)\n",
    "    fig.set_size_inches(15, 5)\n",
    "    \n",
    "    # Kmeans algorithm\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    labels = kmeans.fit_predict(Features)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    silhouetteSamples = silhouette_samples(Features, labels)\n",
    "\n",
    "    # Silhouette graph\n",
    "    yLlower, yUpper = 0, 0\n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        vals = silhouetteSamples[labels == cluster]\n",
    "        silhouetteSamples.sort()\n",
    "        yUpper += len(vals)\n",
    "        ax1.barh(range(yLlower, yUpper), vals, edgecolor='none', height=1)\n",
    "        ax1.text(-0.03, (yLlower + yUpper) / 2, str(i + 1))\n",
    "        yLlower += len(vals)\n",
    "\n",
    "    ave = np.mean(silhouetteSamples)\n",
    "    ax1.axvline(ave, linestyle='--', color='green')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_xlabel('Silhouette coefficient values')\n",
    "    ax1.set_ylabel('Cluster labels')\n",
    "    ax1.set_title('Silhouette coefficient values for different clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to cluster the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Bi:Topics</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['canadian', 'occidental', 'petroleum', 'ltd',...</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['bank', 'america', 'launch', 'three', 'year',...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['deutsche', 'bank', 'ag', 'management', 'boar...</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['european', 'bourse', 'fell', 'tuesday', 'eve...</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['french', 'share', 'closed', 'lower', 'tuesda...</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Bi:Topics labels\n",
       "0  ['canadian', 'occidental', 'petroleum', 'ltd',...        13      2\n",
       "1  ['bank', 'america', 'launch', 'three', 'year',...        11      2\n",
       "2  ['deutsche', 'bank', 'ag', 'management', 'boar...        12      4\n",
       "3  ['european', 'bourse', 'fell', 'tuesday', 'eve...        64      5\n",
       "4  ['french', 'share', 'closed', 'lower', 'tuesda...        46      5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to create clustres\n",
    "def assignClusterID(dataframe, Features, Target, textColumn, topicColumns, clusteringAlgorithm, n_clusters):\n",
    "    kmeans = clusteringAlgorithm(n_clusters=n_clusters).fit(Features)\n",
    "    labels = kmeans.labels_\n",
    "    array = np.array(labels)                        # attching all the labels for each row in dataframe\n",
    "    dataframe['labels'] = array\n",
    "    dfWithClustersID  = dataframe[[textColumn,topicColumns,'labels']]\n",
    "    return dfWithClustersID\n",
    "Features, Target= featureExtraction(df,'Text','Bi:Topics')\n",
    "\n",
    "# calling cluster function\n",
    "dfWithClustersID = assignClusterID(df, Features, Target, 'Text', 'Bi:Topics', KMeans, 9)\n",
    "\n",
    "# storing cluster ID in csv file to make it resuable\n",
    "import pandas as pd\n",
    "dfWithClustersID.to_csv(\"./target.csv\")\n",
    "dfWithClustersID = pd.read_csv(\"./target.csv\")\n",
    "\n",
    "dfWithClustersID = dfWithClustersID.iloc[:,:3]\n",
    "dfWithClustersID.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SVM to each cluster to classify the documents\n",
    "\n",
    "SVM is giving the best result so far. I tried multiple algorithms like linear regression, decision tree, random forest and even neural network. However, SVM works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Shape (1944, 5930)\n",
      "The Accuracy for SVM model is 0.9212328767123288\n",
      "Features Shape (2875, 12147)\n",
      "The Accuracy for SVM model is 0.8551564310544612\n",
      "Features Shape (21333, 63975)\n",
      "The Accuracy for SVM model is 0.75546875\n",
      "Features Shape (1074, 2544)\n",
      "The Accuracy for SVM model is 1.0\n",
      "Features Shape (12004, 53902)\n",
      "The Accuracy for SVM model is 0.7387562465297057\n",
      "Features Shape (6363, 21505)\n",
      "The Accuracy for SVM model is 0.8124672603457308\n",
      "Features Shape (1373, 5080)\n",
      "The Accuracy for SVM model is 0.9757281553398058\n",
      "Features Shape (818, 1475)\n",
      "The Accuracy for SVM model is 1.0\n",
      "Features Shape (591, 7196)\n",
      "The Accuracy for SVM model is 0.9438202247191011\n"
     ]
    }
   ],
   "source": [
    "dict_of_companies = {k: v for k, v in dfWithClustersID.groupby('labels')}\n",
    "smAccuracyList = []\n",
    "for k in dict_of_companies:\n",
    "    Features,Target = preprocessing(dict_of_companies[k],'Text', 'Bi:Topics')\n",
    "    predicted, y_test = generateClassifier(Features, Target, SVC)\n",
    "    accuracyScoreSVC = evaluateModel(y_test, predicted)\n",
    "    smAccuracyList.append(accuracyScoreSVC)\n",
    "    print(\"The Accuracy for SVM model is\", accuracyScoreSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation of the clustering\n",
    "\n",
    "I used Elblow method and Silhouette(which is implemented above) to evaluate the clustering. These evaluation matrix can be used to decide the efficient value of K and also to evaluate the performance k.\n",
    "\n",
    "There is no perfect evaluation method for clustering, we can compare different values of K with each other and decide which works the best for us. thats excatly what these two evaluation methods do.\n",
    "\n",
    "Elbow method gives an idea of optimum value for k based on Sum Squared Error/Distance between centroids and data points, we pick the k value where elbow shape occures, but with these data it does not happenes accurately till k=20(I wouldnt go above 20 because of the need of sufficient data to feed the model). But I can make sure from this graph that SSE decreases as k-value increases. So K= 9 is better than k=5. To choose the value of K, I plotted Silhouette coefficient graph to get better insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluateClustering(kValue)\n",
    "    sse = []\n",
    "    for k in kValue:\n",
    "        print(k)\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km.fit(Features)\n",
    "        sse.append(km.inertia_)\n",
    "\n",
    "    # Plot sse against k\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.plot(list_k, sse, '-o')\n",
    "    plt.xlabel('Number of clusters k')\n",
    "    plt.ylabel('Sum of squared distance');\n",
    "kvalueList[2,4,6,8,10,12,14,16,18]    \n",
    "evaluateClustering(kvalueList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Selection & Extraction and classifying the document using deep nueral network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction using Autoencoder \n",
    "\n",
    "Two main purpose of Feature selection is to reduce the vocabulary size to train the classifier more efficiently(specially with the classifiers which are expensive to train). The second one is improve the accuracy by avoiding the noise.\n",
    "\n",
    "Here, My main purpose of feature selection is to train expensive classifiers effectively by reducing the number of the features. According to the first research papaer, I am using one of the mentioned method 'AutoEncoder' for feature extraction in text classifiction. \n",
    "\n",
    "Autoencoder can be used for dimensionality reduction and denoising the data. In this case, Autoencoder reduces the size of the vocabulary. Here I am using Deep autoencoder with few hidden layes as given below. The vocabulary size is reduced to 512 for each cluster. I also added regularization term to avoid overfitting. \n",
    "\n",
    "### Classification using SVM\n",
    "\n",
    "For classification, SVM and MLP are used. For me, SVM model is working really well than MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1344 samples, validate on 577 samples\n",
      "Epoch 1/50\n",
      "1344/1344 [==============================] - 4s 3ms/step - loss: 1.4457 - val_loss: 1.1987\n",
      "Epoch 2/50\n",
      "1344/1344 [==============================] - 0s 50us/step - loss: 1.1488 - val_loss: 1.0256\n",
      "Epoch 3/50\n",
      "1344/1344 [==============================] - 0s 51us/step - loss: 0.9953 - val_loss: 0.9131\n",
      "Epoch 4/50\n",
      "1344/1344 [==============================] - 0s 51us/step - loss: 0.8913 - val_loss: 0.8317\n",
      "Epoch 5/50\n",
      "1344/1344 [==============================] - 0s 45us/step - loss: 0.8160 - val_loss: 0.7712\n",
      "Epoch 6/50\n",
      "1344/1344 [==============================] - 0s 50us/step - loss: 0.7592 - val_loss: 0.7243\n",
      "Epoch 7/50\n",
      "1344/1344 [==============================] - 0s 49us/step - loss: 0.7151 - val_loss: 0.6882\n",
      "Epoch 8/50\n",
      "1344/1344 [==============================] - 0s 52us/step - loss: 0.6811 - val_loss: 0.6596\n",
      "Epoch 9/50\n",
      "1344/1344 [==============================] - 0s 45us/step - loss: 0.6538 - val_loss: 0.6352\n",
      "Epoch 10/50\n",
      "1344/1344 [==============================] - 0s 50us/step - loss: 0.6299 - val_loss: 0.6129\n",
      "Epoch 11/50\n",
      "1344/1344 [==============================] - 0s 49us/step - loss: 0.6075 - val_loss: 0.5907\n",
      "Epoch 12/50\n",
      "1344/1344 [==============================] - 0s 48us/step - loss: 0.5851 - val_loss: 0.5678\n",
      "Epoch 13/50\n",
      "1344/1344 [==============================] - 0s 49us/step - loss: 0.5617 - val_loss: 0.5435\n",
      "Epoch 14/50\n",
      "1344/1344 [==============================] - 0s 49us/step - loss: 0.5369 - val_loss: 0.5178\n",
      "Epoch 15/50\n",
      "1344/1344 [==============================] - 0s 50us/step - loss: 0.5108 - val_loss: 0.4910\n",
      "Epoch 16/50\n",
      "1344/1344 [==============================] - 0s 53us/step - loss: 0.4837 - val_loss: 0.4639\n",
      "Epoch 17/50\n",
      "1344/1344 [==============================] - 0s 55us/step - loss: 0.4565 - val_loss: 0.4371\n",
      "Epoch 18/50\n",
      "1344/1344 [==============================] - 0s 54us/step - loss: 0.4299 - val_loss: 0.4117\n",
      "Epoch 19/50\n",
      "1344/1344 [==============================] - 0s 58us/step - loss: 0.4050 - val_loss: 0.3882\n",
      "Epoch 20/50\n",
      "1344/1344 [==============================] - 0s 47us/step - loss: 0.3821 - val_loss: 0.3672\n",
      "Epoch 21/50\n",
      "1344/1344 [==============================] - 0s 52us/step - loss: 0.3618 - val_loss: 0.3489\n",
      "Epoch 22/50\n",
      "1344/1344 [==============================] - 0s 48us/step - loss: 0.3441 - val_loss: 0.3332\n",
      "Epoch 23/50\n",
      "1344/1344 [==============================] - 0s 48us/step - loss: 0.3291 - val_loss: 0.3201\n",
      "Epoch 24/50\n",
      "1344/1344 [==============================] - 0s 47us/step - loss: 0.3166 - val_loss: 0.3092\n",
      "Epoch 25/50\n",
      "1344/1344 [==============================] - 0s 57us/step - loss: 0.3062 - val_loss: 0.3002\n",
      "Epoch 26/50\n",
      "1344/1344 [==============================] - 0s 53us/step - loss: 0.2977 - val_loss: 0.2929\n",
      "Epoch 27/50\n",
      "1344/1344 [==============================] - 0s 53us/step - loss: 0.2907 - val_loss: 0.2870\n",
      "Epoch 28/50\n",
      "1344/1344 [==============================] - 0s 56us/step - loss: 0.2850 - val_loss: 0.2821\n",
      "Epoch 29/50\n",
      "1344/1344 [==============================] - 0s 56us/step - loss: 0.2804 - val_loss: 0.2781\n",
      "Epoch 30/50\n",
      "1344/1344 [==============================] - 0s 50us/step - loss: 0.2765 - val_loss: 0.2748\n",
      "Epoch 31/50\n",
      "1344/1344 [==============================] - 0s 59us/step - loss: 0.2734 - val_loss: 0.2721\n",
      "Epoch 32/50\n",
      "1344/1344 [==============================] - 0s 60us/step - loss: 0.2708 - val_loss: 0.2699\n",
      "Epoch 33/50\n",
      "1344/1344 [==============================] - 0s 58us/step - loss: 0.2686 - val_loss: 0.2680\n",
      "Epoch 34/50\n",
      "1344/1344 [==============================] - 0s 51us/step - loss: 0.2668 - val_loss: 0.2665\n",
      "Epoch 35/50\n",
      "1344/1344 [==============================] - 0s 56us/step - loss: 0.2652 - val_loss: 0.2651\n",
      "Epoch 36/50\n",
      "1344/1344 [==============================] - 0s 57us/step - loss: 0.2639 - val_loss: 0.2640\n",
      "Epoch 37/50\n",
      "1344/1344 [==============================] - 0s 52us/step - loss: 0.2628 - val_loss: 0.2630\n",
      "Epoch 38/50\n",
      "1344/1344 [==============================] - 0s 62us/step - loss: 0.2619 - val_loss: 0.2622\n",
      "Epoch 39/50\n",
      "1344/1344 [==============================] - 0s 58us/step - loss: 0.2610 - val_loss: 0.2614\n",
      "Epoch 40/50\n",
      "1344/1344 [==============================] - 0s 56us/step - loss: 0.2603 - val_loss: 0.2608\n",
      "Epoch 41/50\n",
      "1344/1344 [==============================] - 0s 52us/step - loss: 0.2597 - val_loss: 0.2602\n",
      "Epoch 42/50\n",
      "1344/1344 [==============================] - 0s 65us/step - loss: 0.2591 - val_loss: 0.2597\n",
      "Epoch 43/50\n",
      "1344/1344 [==============================] - 0s 52us/step - loss: 0.2586 - val_loss: 0.2593\n",
      "Epoch 44/50\n",
      "1344/1344 [==============================] - 0s 47us/step - loss: 0.2581 - val_loss: 0.2589\n",
      "Epoch 45/50\n",
      "1344/1344 [==============================] - 0s 45us/step - loss: 0.2578 - val_loss: 0.2585\n",
      "Epoch 46/50\n",
      "1344/1344 [==============================] - 0s 46us/step - loss: 0.2574 - val_loss: 0.2582\n",
      "Epoch 47/50\n",
      "1344/1344 [==============================] - 0s 49us/step - loss: 0.2571 - val_loss: 0.2579\n",
      "Epoch 48/50\n",
      "1344/1344 [==============================] - 0s 49us/step - loss: 0.2568 - val_loss: 0.2576\n",
      "Epoch 49/50\n",
      "1344/1344 [==============================] - 0s 47us/step - loss: 0.2565 - val_loss: 0.2574\n",
      "Epoch 50/50\n",
      "1344/1344 [==============================] - 0s 49us/step - loss: 0.2563 - val_loss: 0.2572\n",
      "The Accuracy for SVM model is %.2f 0.43154246100519933\n",
      "Train on 1948 samples, validate on 836 samples\n",
      "Epoch 1/50\n",
      "1948/1948 [==============================] - 4s 2ms/step - loss: 1.3281 - val_loss: 1.0463\n",
      "Epoch 2/50\n",
      "1948/1948 [==============================] - 0s 49us/step - loss: 1.0184 - val_loss: 0.8892\n",
      "Epoch 3/50\n",
      "1948/1948 [==============================] - 0s 54us/step - loss: 0.8735 - val_loss: 0.7958\n",
      "Epoch 4/50\n",
      "1948/1948 [==============================] - 0s 53us/step - loss: 0.7838 - val_loss: 0.7324\n",
      "Epoch 5/50\n",
      "1948/1948 [==============================] - 0s 54us/step - loss: 0.7221 - val_loss: 0.6880\n",
      "Epoch 6/50\n",
      "1948/1948 [==============================] - 0s 50us/step - loss: 0.6796 - val_loss: 0.6552\n",
      "Epoch 7/50\n",
      "1948/1948 [==============================] - 0s 44us/step - loss: 0.6469 - val_loss: 0.6273\n",
      "Epoch 8/50\n",
      "1948/1948 [==============================] - 0s 45us/step - loss: 0.6191 - val_loss: 0.6011\n",
      "Epoch 9/50\n",
      "1948/1948 [==============================] - 0s 49us/step - loss: 0.5919 - val_loss: 0.5730\n",
      "Epoch 10/50\n",
      "1948/1948 [==============================] - 0s 48us/step - loss: 0.5625 - val_loss: 0.5418\n",
      "Epoch 11/50\n",
      "1948/1948 [==============================] - 0s 45us/step - loss: 0.5302 - val_loss: 0.5085\n",
      "Epoch 12/50\n",
      "1948/1948 [==============================] - 0s 46us/step - loss: 0.4964 - val_loss: 0.4741\n",
      "Epoch 13/50\n",
      "1948/1948 [==============================] - 0s 45us/step - loss: 0.4620 - val_loss: 0.4401\n",
      "Epoch 14/50\n",
      "1948/1948 [==============================] - 0s 44us/step - loss: 0.4285 - val_loss: 0.4082\n",
      "Epoch 15/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.3979 - val_loss: 0.3800\n",
      "Epoch 16/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.3712 - val_loss: 0.3563\n",
      "Epoch 17/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.3492 - val_loss: 0.3371\n",
      "Epoch 18/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.3316 - val_loss: 0.3222\n",
      "Epoch 19/50\n",
      "1948/1948 [==============================] - 0s 53us/step - loss: 0.3179 - val_loss: 0.3106\n",
      "Epoch 20/50\n",
      "1948/1948 [==============================] - 0s 48us/step - loss: 0.3074 - val_loss: 0.3019\n",
      "Epoch 21/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2994 - val_loss: 0.2952\n",
      "Epoch 22/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2934 - val_loss: 0.2901\n",
      "Epoch 23/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2887 - val_loss: 0.2862\n",
      "Epoch 24/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.2851 - val_loss: 0.2832\n",
      "Epoch 25/50\n",
      "1948/1948 [==============================] - 0s 45us/step - loss: 0.2824 - val_loss: 0.2808\n",
      "Epoch 26/50\n",
      "1948/1948 [==============================] - 0s 46us/step - loss: 0.2802 - val_loss: 0.2789\n",
      "Epoch 27/50\n",
      "1948/1948 [==============================] - 0s 44us/step - loss: 0.2784 - val_loss: 0.2774\n",
      "Epoch 28/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2770 - val_loss: 0.2761\n",
      "Epoch 29/50\n",
      "1948/1948 [==============================] - 0s 40us/step - loss: 0.2758 - val_loss: 0.2751\n",
      "Epoch 30/50\n",
      "1948/1948 [==============================] - 0s 45us/step - loss: 0.2749 - val_loss: 0.2743\n",
      "Epoch 31/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2741 - val_loss: 0.2735\n",
      "Epoch 32/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2734 - val_loss: 0.2729\n",
      "Epoch 33/50\n",
      "1948/1948 [==============================] - 0s 40us/step - loss: 0.2728 - val_loss: 0.2724\n",
      "Epoch 34/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.2723 - val_loss: 0.2720\n",
      "Epoch 35/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2719 - val_loss: 0.2716\n",
      "Epoch 36/50\n",
      "1948/1948 [==============================] - 0s 39us/step - loss: 0.2715 - val_loss: 0.2712\n",
      "Epoch 37/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.2712 - val_loss: 0.2709\n",
      "Epoch 38/50\n",
      "1948/1948 [==============================] - 0s 40us/step - loss: 0.2709 - val_loss: 0.2707\n",
      "Epoch 39/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2707 - val_loss: 0.2704\n",
      "Epoch 40/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2705 - val_loss: 0.2702\n",
      "Epoch 41/50\n",
      "1948/1948 [==============================] - 0s 43us/step - loss: 0.2702 - val_loss: 0.2700\n",
      "Epoch 42/50\n",
      "1948/1948 [==============================] - 0s 39us/step - loss: 0.2701 - val_loss: 0.2699\n",
      "Epoch 43/50\n",
      "1948/1948 [==============================] - 0s 42us/step - loss: 0.2699 - val_loss: 0.2697\n",
      "Epoch 44/50\n",
      "1948/1948 [==============================] - 0s 39us/step - loss: 0.2698 - val_loss: 0.2696\n",
      "Epoch 45/50\n",
      "1948/1948 [==============================] - 0s 46us/step - loss: 0.2696 - val_loss: 0.2694\n",
      "Epoch 46/50\n",
      "1948/1948 [==============================] - 0s 39us/step - loss: 0.2695 - val_loss: 0.2693\n",
      "Epoch 47/50\n",
      "1948/1948 [==============================] - 0s 40us/step - loss: 0.2694 - val_loss: 0.2692\n",
      "Epoch 48/50\n",
      "1948/1948 [==============================] - 0s 40us/step - loss: 0.2693 - val_loss: 0.2691\n",
      "Epoch 49/50\n",
      "1948/1948 [==============================] - 0s 40us/step - loss: 0.2692 - val_loss: 0.2690\n",
      "Epoch 50/50\n",
      "1948/1948 [==============================] - 0s 41us/step - loss: 0.2691 - val_loss: 0.2689\n",
      "The Accuracy for SVM model is %.2f 0.7380382775119617\n",
      "Train on 14416 samples, validate on 6179 samples\n",
      "Epoch 1/50\n",
      "14416/14416 [==============================] - 5s 327us/step - loss: 0.8549 - val_loss: 0.6272\n",
      "Epoch 2/50\n",
      "14416/14416 [==============================] - 1s 41us/step - loss: 0.5231 - val_loss: 0.3972\n",
      "Epoch 3/50\n",
      "14416/14416 [==============================] - 1s 41us/step - loss: 0.3328 - val_loss: 0.2925\n",
      "Epoch 4/50\n",
      "14416/14416 [==============================] - 1s 42us/step - loss: 0.2825 - val_loss: 0.2760\n",
      "Epoch 5/50\n",
      "14416/14416 [==============================] - 1s 39us/step - loss: 0.2736 - val_loss: 0.2719\n",
      "Epoch 6/50\n",
      "14416/14416 [==============================] - 1s 37us/step - loss: 0.2710 - val_loss: 0.2703\n",
      "Epoch 7/50\n",
      "14416/14416 [==============================] - 1s 39us/step - loss: 0.2698 - val_loss: 0.2695\n",
      "Epoch 8/50\n",
      "14416/14416 [==============================] - 1s 38us/step - loss: 0.2692 - val_loss: 0.2691\n",
      "Epoch 9/50\n",
      "14416/14416 [==============================] - 1s 40us/step - loss: 0.2688 - val_loss: 0.2688\n",
      "Epoch 10/50\n",
      "14416/14416 [==============================] - 1s 40us/step - loss: 0.2686 - val_loss: 0.2686\n",
      "Epoch 11/50\n",
      "14416/14416 [==============================] - 1s 42us/step - loss: 0.2685 - val_loss: 0.2685\n",
      "Epoch 12/50\n",
      "14416/14416 [==============================] - 1s 41us/step - loss: 0.2684 - val_loss: 0.2684\n",
      "Epoch 13/50\n",
      "14416/14416 [==============================] - 1s 40us/step - loss: 0.2683 - val_loss: 0.2684\n",
      "Epoch 14/50\n",
      "14416/14416 [==============================] - 1s 39us/step - loss: 0.2683 - val_loss: 0.2683\n",
      "Epoch 15/50\n",
      "14416/14416 [==============================] - 1s 50us/step - loss: 0.2682 - val_loss: 0.2683\n",
      "Epoch 16/50\n",
      "14416/14416 [==============================] - 1s 56us/step - loss: 0.2682 - val_loss: 0.2683\n",
      "Epoch 17/50\n",
      "14416/14416 [==============================] - 1s 57us/step - loss: 0.2682 - val_loss: 0.2683\n",
      "Epoch 18/50\n",
      "14416/14416 [==============================] - 1s 40us/step - loss: 0.2682 - val_loss: 0.2682\n",
      "Epoch 19/50\n",
      "14416/14416 [==============================] - 1s 54us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 20/50\n",
      "14416/14416 [==============================] - 1s 56us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 21/50\n",
      "14416/14416 [==============================] - 1s 46us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 22/50\n",
      "14416/14416 [==============================] - 1s 46us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 23/50\n",
      "14416/14416 [==============================] - 1s 45us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 24/50\n",
      "14416/14416 [==============================] - 1s 45us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 25/50\n",
      "14416/14416 [==============================] - 1s 43us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 26/50\n",
      "14416/14416 [==============================] - 1s 49us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 27/50\n",
      "14416/14416 [==============================] - 1s 42us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 28/50\n",
      "14416/14416 [==============================] - 1s 51us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 29/50\n",
      "14416/14416 [==============================] - 1s 41us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 30/50\n",
      "14416/14416 [==============================] - 1s 43us/step - loss: 0.2681 - val_loss: 0.2682\n",
      "Epoch 31/50\n",
      "14416/14416 [==============================] - 1s 57us/step - loss: 0.2680 - val_loss: 0.2682\n",
      "Epoch 32/50\n",
      "14416/14416 [==============================] - 1s 54us/step - loss: 0.2680 - val_loss: 0.2682\n",
      "Epoch 33/50\n",
      "14416/14416 [==============================] - 1s 60us/step - loss: 0.2680 - val_loss: 0.2682\n",
      "Epoch 34/50\n",
      "14416/14416 [==============================] - 1s 43us/step - loss: 0.2680 - val_loss: 0.2682\n",
      "Epoch 35/50\n",
      "14416/14416 [==============================] - 1s 47us/step - loss: 0.2680 - val_loss: 0.2682\n",
      "Epoch 36/50\n",
      "14416/14416 [==============================] - 1s 53us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 37/50\n",
      "14416/14416 [==============================] - 1s 50us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 38/50\n",
      "14416/14416 [==============================] - 1s 51us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 39/50\n",
      "14416/14416 [==============================] - 1s 41us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 40/50\n",
      "14416/14416 [==============================] - 1s 44us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 41/50\n",
      "14416/14416 [==============================] - 1s 44us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 42/50\n",
      "14416/14416 [==============================] - 1s 50us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 43/50\n",
      "14416/14416 [==============================] - 1s 54us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 44/50\n",
      "14416/14416 [==============================] - 1s 48us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 45/50\n",
      "14416/14416 [==============================] - 1s 50us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 46/50\n",
      "14416/14416 [==============================] - 1s 44us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 47/50\n",
      "14416/14416 [==============================] - 1s 41us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 48/50\n",
      "14416/14416 [==============================] - 1s 43us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 49/50\n",
      "14416/14416 [==============================] - 1s 40us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "Epoch 50/50\n",
      "14416/14416 [==============================] - 1s 42us/step - loss: 0.2680 - val_loss: 0.2681\n",
      "The Accuracy for SVM model is %.2f 0.19372066677455899\n",
      "Train on 705 samples, validate on 303 samples\n",
      "Epoch 1/50\n",
      "705/705 [==============================] - 4s 6ms/step - loss: 1.4081 - val_loss: 1.0699\n",
      "Epoch 2/50\n",
      "705/705 [==============================] - 0s 57us/step - loss: 1.2111 - val_loss: 0.9806\n",
      "Epoch 3/50\n",
      "705/705 [==============================] - 0s 55us/step - loss: 1.0911 - val_loss: 0.9199\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 0s 52us/step - loss: 1.0087 - val_loss: 0.8732\n",
      "Epoch 5/50\n",
      "705/705 [==============================] - 0s 68us/step - loss: 0.9454 - val_loss: 0.8351\n",
      "Epoch 6/50\n",
      "705/705 [==============================] - 0s 69us/step - loss: 0.8944 - val_loss: 0.8026\n",
      "Epoch 7/50\n",
      "705/705 [==============================] - 0s 69us/step - loss: 0.8513 - val_loss: 0.7744\n",
      "Epoch 8/50\n",
      "705/705 [==============================] - 0s 89us/step - loss: 0.8137 - val_loss: 0.7491\n",
      "Epoch 9/50\n",
      "705/705 [==============================] - 0s 69us/step - loss: 0.7806 - val_loss: 0.7260\n",
      "Epoch 10/50\n",
      "705/705 [==============================] - 0s 64us/step - loss: 0.7510 - val_loss: 0.7050\n",
      "Epoch 11/50\n",
      "705/705 [==============================] - 0s 58us/step - loss: 0.7244 - val_loss: 0.6860\n",
      "Epoch 12/50\n",
      "705/705 [==============================] - 0s 57us/step - loss: 0.7007 - val_loss: 0.6689\n",
      "Epoch 13/50\n",
      "705/705 [==============================] - 0s 58us/step - loss: 0.6801 - val_loss: 0.6531\n",
      "Epoch 14/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.6613 - val_loss: 0.6382\n",
      "Epoch 15/50\n",
      "705/705 [==============================] - 0s 69us/step - loss: 0.6443 - val_loss: 0.6238\n",
      "Epoch 16/50\n",
      "705/705 [==============================] - 0s 58us/step - loss: 0.6281 - val_loss: 0.6094\n",
      "Epoch 17/50\n",
      "705/705 [==============================] - 0s 56us/step - loss: 0.6123 - val_loss: 0.5947\n",
      "Epoch 18/50\n",
      "705/705 [==============================] - 0s 55us/step - loss: 0.5964 - val_loss: 0.5796\n",
      "Epoch 19/50\n",
      "705/705 [==============================] - 0s 64us/step - loss: 0.5805 - val_loss: 0.5641\n",
      "Epoch 20/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.5643 - val_loss: 0.5481\n",
      "Epoch 21/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.5477 - val_loss: 0.5315\n",
      "Epoch 22/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.5307 - val_loss: 0.5144\n",
      "Epoch 23/50\n",
      "705/705 [==============================] - 0s 51us/step - loss: 0.5131 - val_loss: 0.4969\n",
      "Epoch 24/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.4954 - val_loss: 0.4792\n",
      "Epoch 25/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.4775 - val_loss: 0.4616\n",
      "Epoch 26/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.4598 - val_loss: 0.4442\n",
      "Epoch 27/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.4423 - val_loss: 0.4273\n",
      "Epoch 28/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.4255 - val_loss: 0.4111\n",
      "Epoch 29/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.4094 - val_loss: 0.3958\n",
      "Epoch 30/50\n",
      "705/705 [==============================] - 0s 61us/step - loss: 0.3941 - val_loss: 0.3814\n",
      "Epoch 31/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.3799 - val_loss: 0.3681\n",
      "Epoch 32/50\n",
      "705/705 [==============================] - 0s 57us/step - loss: 0.3667 - val_loss: 0.3559\n",
      "Epoch 33/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.3546 - val_loss: 0.3449\n",
      "Epoch 34/50\n",
      "705/705 [==============================] - 0s 55us/step - loss: 0.3438 - val_loss: 0.3350\n",
      "Epoch 35/50\n",
      "705/705 [==============================] - 0s 58us/step - loss: 0.3340 - val_loss: 0.3261\n",
      "Epoch 36/50\n",
      "705/705 [==============================] - 0s 55us/step - loss: 0.3253 - val_loss: 0.3183\n",
      "Epoch 37/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.3176 - val_loss: 0.3114\n",
      "Epoch 38/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.3107 - val_loss: 0.3053\n",
      "Epoch 39/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.3047 - val_loss: 0.2999\n",
      "Epoch 40/50\n",
      "705/705 [==============================] - 0s 52us/step - loss: 0.2995 - val_loss: 0.2952\n",
      "Epoch 41/50\n",
      "705/705 [==============================] - 0s 57us/step - loss: 0.2948 - val_loss: 0.2911\n",
      "Epoch 42/50\n",
      "705/705 [==============================] - 0s 54us/step - loss: 0.2908 - val_loss: 0.2875\n",
      "Epoch 43/50\n",
      "705/705 [==============================] - 0s 50us/step - loss: 0.2872 - val_loss: 0.2843\n",
      "Epoch 44/50\n",
      "705/705 [==============================] - 0s 55us/step - loss: 0.2841 - val_loss: 0.2815\n",
      "Epoch 45/50\n",
      "705/705 [==============================] - 0s 55us/step - loss: 0.2813 - val_loss: 0.2791\n",
      "Epoch 46/50\n",
      "705/705 [==============================] - 0s 72us/step - loss: 0.2790 - val_loss: 0.2769\n",
      "Epoch 47/50\n",
      "705/705 [==============================] - 0s 76us/step - loss: 0.2768 - val_loss: 0.2750\n",
      "Epoch 48/50\n",
      "705/705 [==============================] - 0s 61us/step - loss: 0.2749 - val_loss: 0.2733\n",
      "Epoch 49/50\n",
      "705/705 [==============================] - 0s 69us/step - loss: 0.2733 - val_loss: 0.2718\n",
      "Epoch 50/50\n",
      "705/705 [==============================] - 0s 65us/step - loss: 0.2718 - val_loss: 0.2705\n",
      "The Accuracy for SVM model is %.2f 1.0\n",
      "Train on 8226 samples, validate on 3526 samples\n",
      "Epoch 1/50\n",
      "8226/8226 [==============================] - 4s 495us/step - loss: 0.9790 - val_loss: 0.7140\n",
      "Epoch 2/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.6510 - val_loss: 0.5878\n",
      "Epoch 3/50\n",
      "8226/8226 [==============================] - 0s 43us/step - loss: 0.5325 - val_loss: 0.4558\n",
      "Epoch 4/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.3994 - val_loss: 0.3424\n",
      "Epoch 5/50\n",
      "8226/8226 [==============================] - 0s 43us/step - loss: 0.3175 - val_loss: 0.2962\n",
      "Epoch 6/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2880 - val_loss: 0.2809\n",
      "Epoch 7/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2778 - val_loss: 0.2750\n",
      "Epoch 8/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2735 - val_loss: 0.2722\n",
      "Epoch 9/50\n",
      "8226/8226 [==============================] - 0s 40us/step - loss: 0.2713 - val_loss: 0.2706\n",
      "Epoch 10/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2701 - val_loss: 0.2696\n",
      "Epoch 11/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2693 - val_loss: 0.2690\n",
      "Epoch 12/50\n",
      "8226/8226 [==============================] - 0s 43us/step - loss: 0.2687 - val_loss: 0.2686\n",
      "Epoch 13/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2683 - val_loss: 0.2683\n",
      "Epoch 14/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2681 - val_loss: 0.2680\n",
      "Epoch 15/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2678 - val_loss: 0.2678\n",
      "Epoch 16/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2677 - val_loss: 0.2677\n",
      "Epoch 17/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2676 - val_loss: 0.2676\n",
      "Epoch 18/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2675 - val_loss: 0.2675\n",
      "Epoch 19/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2674 - val_loss: 0.2674\n",
      "Epoch 20/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2673 - val_loss: 0.2674\n",
      "Epoch 21/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2673 - val_loss: 0.2673\n",
      "Epoch 22/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2672 - val_loss: 0.2673\n",
      "Epoch 23/50\n",
      "8226/8226 [==============================] - 0s 46us/step - loss: 0.2672 - val_loss: 0.2672\n",
      "Epoch 24/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2671 - val_loss: 0.2672\n",
      "Epoch 25/50\n",
      "8226/8226 [==============================] - 0s 43us/step - loss: 0.2671 - val_loss: 0.2672\n",
      "Epoch 26/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2671 - val_loss: 0.2672\n",
      "Epoch 27/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2671 - val_loss: 0.2671\n",
      "Epoch 28/50\n",
      "8226/8226 [==============================] - 0s 43us/step - loss: 0.2670 - val_loss: 0.2671\n",
      "Epoch 29/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2670 - val_loss: 0.2671\n",
      "Epoch 30/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2670 - val_loss: 0.2671\n",
      "Epoch 31/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2670 - val_loss: 0.2671\n",
      "Epoch 32/50\n",
      "8226/8226 [==============================] - 0s 44us/step - loss: 0.2670 - val_loss: 0.2671\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2670 - val_loss: 0.2670\n",
      "Epoch 34/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 35/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 36/50\n",
      "8226/8226 [==============================] - 0s 40us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 37/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 38/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 39/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 40/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 41/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 42/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 43/50\n",
      "8226/8226 [==============================] - 0s 44us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 44/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 45/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 46/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 47/50\n",
      "8226/8226 [==============================] - 0s 43us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 48/50\n",
      "8226/8226 [==============================] - 0s 41us/step - loss: 0.2669 - val_loss: 0.2670\n",
      "Epoch 49/50\n",
      "8226/8226 [==============================] - 0s 43us/step - loss: 0.2669 - val_loss: 0.2669\n",
      "Epoch 50/50\n",
      "8226/8226 [==============================] - 0s 42us/step - loss: 0.2669 - val_loss: 0.2669\n",
      "The Accuracy for SVM model is %.2f 0.4671015314804311\n",
      "Train on 4301 samples, validate on 1844 samples\n",
      "Epoch 1/50\n",
      "4301/4301 [==============================] - 4s 866us/step - loss: 1.1999 - val_loss: 0.8850\n",
      "Epoch 2/50\n",
      "4301/4301 [==============================] - 0s 43us/step - loss: 0.7973 - val_loss: 0.7050\n",
      "Epoch 3/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.6728 - val_loss: 0.6329\n",
      "Epoch 4/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.6090 - val_loss: 0.5743\n",
      "Epoch 5/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.5464 - val_loss: 0.5036\n",
      "Epoch 6/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.4718 - val_loss: 0.4271\n",
      "Epoch 7/50\n",
      "4301/4301 [==============================] - 0s 44us/step - loss: 0.3991 - val_loss: 0.3633\n",
      "Epoch 8/50\n",
      "4301/4301 [==============================] - 0s 49us/step - loss: 0.3443 - val_loss: 0.3216\n",
      "Epoch 9/50\n",
      "4301/4301 [==============================] - 0s 46us/step - loss: 0.3106 - val_loss: 0.2980\n",
      "Epoch 10/50\n",
      "4301/4301 [==============================] - 0s 44us/step - loss: 0.2920 - val_loss: 0.2851\n",
      "Epoch 11/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2817 - val_loss: 0.2779\n",
      "Epoch 12/50\n",
      "4301/4301 [==============================] - 0s 44us/step - loss: 0.2759 - val_loss: 0.2737\n",
      "Epoch 13/50\n",
      "4301/4301 [==============================] - 0s 43us/step - loss: 0.2724 - val_loss: 0.2710\n",
      "Epoch 14/50\n",
      "4301/4301 [==============================] - 0s 43us/step - loss: 0.2702 - val_loss: 0.2693\n",
      "Epoch 15/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2687 - val_loss: 0.2681\n",
      "Epoch 16/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2676 - val_loss: 0.2672\n",
      "Epoch 17/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2668 - val_loss: 0.2665\n",
      "Epoch 18/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2662 - val_loss: 0.2660\n",
      "Epoch 19/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2657 - val_loss: 0.2656\n",
      "Epoch 20/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2654 - val_loss: 0.2653\n",
      "Epoch 21/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2651 - val_loss: 0.2650\n",
      "Epoch 22/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2648 - val_loss: 0.2648\n",
      "Epoch 23/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2646 - val_loss: 0.2646\n",
      "Epoch 24/50\n",
      "4301/4301 [==============================] - 0s 46us/step - loss: 0.2645 - val_loss: 0.2645\n",
      "Epoch 25/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2643 - val_loss: 0.2643\n",
      "Epoch 26/50\n",
      "4301/4301 [==============================] - 0s 44us/step - loss: 0.2642 - val_loss: 0.2642\n",
      "Epoch 27/50\n",
      "4301/4301 [==============================] - 0s 43us/step - loss: 0.2641 - val_loss: 0.2641\n",
      "Epoch 28/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2640 - val_loss: 0.2640\n",
      "Epoch 29/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2639 - val_loss: 0.2640\n",
      "Epoch 30/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2638 - val_loss: 0.2639\n",
      "Epoch 31/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2638 - val_loss: 0.2638\n",
      "Epoch 32/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2637 - val_loss: 0.2638\n",
      "Epoch 33/50\n",
      "4301/4301 [==============================] - 0s 43us/step - loss: 0.2637 - val_loss: 0.2638\n",
      "Epoch 34/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2636 - val_loss: 0.2637\n",
      "Epoch 35/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2636 - val_loss: 0.2637\n",
      "Epoch 36/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2636 - val_loss: 0.2636\n",
      "Epoch 37/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 38/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 39/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2635 - val_loss: 0.2636\n",
      "Epoch 40/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2635 - val_loss: 0.2635\n",
      "Epoch 41/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 42/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 43/50\n",
      "4301/4301 [==============================] - 0s 41us/step - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 44/50\n",
      "4301/4301 [==============================] - 0s 44us/step - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 45/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2634 - val_loss: 0.2635\n",
      "Epoch 46/50\n",
      "4301/4301 [==============================] - 0s 43us/step - loss: 0.2633 - val_loss: 0.2634\n",
      "Epoch 47/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2633 - val_loss: 0.2634\n",
      "Epoch 48/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2633 - val_loss: 0.2634\n",
      "Epoch 49/50\n",
      "4301/4301 [==============================] - 0s 43us/step - loss: 0.2633 - val_loss: 0.2634\n",
      "Epoch 50/50\n",
      "4301/4301 [==============================] - 0s 42us/step - loss: 0.2633 - val_loss: 0.2634\n",
      "The Accuracy for SVM model is %.2f 0.2662689804772234\n",
      "Train on 931 samples, validate on 400 samples\n",
      "Epoch 1/50\n",
      "931/931 [==============================] - 4s 4ms/step - loss: 1.4306 - val_loss: 1.1521\n",
      "Epoch 2/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 1.1995 - val_loss: 1.0300\n",
      "Epoch 3/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 1.0691 - val_loss: 0.9512\n",
      "Epoch 4/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.9826 - val_loss: 0.8903\n",
      "Epoch 5/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.9142 - val_loss: 0.8390\n",
      "Epoch 6/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.8565 - val_loss: 0.7951\n",
      "Epoch 7/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.8071 - val_loss: 0.7569\n",
      "Epoch 8/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.7646 - val_loss: 0.7247\n",
      "Epoch 9/50\n",
      "931/931 [==============================] - 0s 50us/step - loss: 0.7294 - val_loss: 0.6989\n",
      "Epoch 10/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.7015 - val_loss: 0.6779\n",
      "Epoch 11/50\n",
      "931/931 [==============================] - 0s 48us/step - loss: 0.6789 - val_loss: 0.6604\n",
      "Epoch 12/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.6604 - val_loss: 0.6456\n",
      "Epoch 13/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.6450 - val_loss: 0.6327\n",
      "Epoch 14/50\n",
      "931/931 [==============================] - 0s 43us/step - loss: 0.6316 - val_loss: 0.6205\n",
      "Epoch 15/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.6190 - val_loss: 0.6083\n",
      "Epoch 16/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.6063 - val_loss: 0.5955\n",
      "Epoch 17/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.5931 - val_loss: 0.5819\n",
      "Epoch 18/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.5793 - val_loss: 0.5677\n",
      "Epoch 19/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.5647 - val_loss: 0.5526\n",
      "Epoch 20/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.5495 - val_loss: 0.5368\n",
      "Epoch 21/50\n",
      "931/931 [==============================] - 0s 43us/step - loss: 0.5334 - val_loss: 0.5202\n",
      "Epoch 22/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.5166 - val_loss: 0.5030\n",
      "Epoch 23/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.4993 - val_loss: 0.4853\n",
      "Epoch 24/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.4815 - val_loss: 0.4674\n",
      "Epoch 25/50\n",
      "931/931 [==============================] - 0s 42us/step - loss: 0.4636 - val_loss: 0.4496\n",
      "Epoch 26/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.4458 - val_loss: 0.4321\n",
      "Epoch 27/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.4284 - val_loss: 0.4151\n",
      "Epoch 28/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.4116 - val_loss: 0.3990\n",
      "Epoch 29/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.3956 - val_loss: 0.3838\n",
      "Epoch 30/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.3807 - val_loss: 0.3697\n",
      "Epoch 31/50\n",
      "931/931 [==============================] - 0s 47us/step - loss: 0.3668 - val_loss: 0.3567\n",
      "Epoch 32/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.3541 - val_loss: 0.3450\n",
      "Epoch 33/50\n",
      "931/931 [==============================] - 0s 49us/step - loss: 0.3427 - val_loss: 0.3345\n",
      "Epoch 34/50\n",
      "931/931 [==============================] - 0s 46us/step - loss: 0.3324 - val_loss: 0.3252\n",
      "Epoch 35/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.3233 - val_loss: 0.3169\n",
      "Epoch 36/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.3153 - val_loss: 0.3096\n",
      "Epoch 37/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.3082 - val_loss: 0.3032\n",
      "Epoch 38/50\n",
      "931/931 [==============================] - 0s 47us/step - loss: 0.3019 - val_loss: 0.2976\n",
      "Epoch 39/50\n",
      "931/931 [==============================] - 0s 47us/step - loss: 0.2965 - val_loss: 0.2927\n",
      "Epoch 40/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.2918 - val_loss: 0.2885\n",
      "Epoch 41/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.2876 - val_loss: 0.2847\n",
      "Epoch 42/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.2840 - val_loss: 0.2815\n",
      "Epoch 43/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.2808 - val_loss: 0.2786\n",
      "Epoch 44/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.2781 - val_loss: 0.2762\n",
      "Epoch 45/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.2756 - val_loss: 0.2740\n",
      "Epoch 46/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.2735 - val_loss: 0.2720\n",
      "Epoch 47/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.2716 - val_loss: 0.2703\n",
      "Epoch 48/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.2700 - val_loss: 0.2688\n",
      "Epoch 49/50\n",
      "931/931 [==============================] - 0s 44us/step - loss: 0.2685 - val_loss: 0.2675\n",
      "Epoch 50/50\n",
      "931/931 [==============================] - 0s 45us/step - loss: 0.2672 - val_loss: 0.2663\n",
      "The Accuracy for SVM model is %.2f 0.9225\n",
      "Train on 515 samples, validate on 221 samples\n",
      "Epoch 1/50\n",
      "515/515 [==============================] - 4s 7ms/step - loss: 1.6263 - val_loss: 1.0118\n",
      "Epoch 2/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 1.4383 - val_loss: 0.9525\n",
      "Epoch 3/50\n",
      "515/515 [==============================] - 0s 66us/step - loss: 1.3064 - val_loss: 0.9088\n",
      "Epoch 4/50\n",
      "515/515 [==============================] - 0s 66us/step - loss: 1.2091 - val_loss: 0.8737\n",
      "Epoch 5/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 1.1317 - val_loss: 0.8440\n",
      "Epoch 6/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 1.0675 - val_loss: 0.8179\n",
      "Epoch 7/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 1.0120 - val_loss: 0.7942\n",
      "Epoch 8/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.9631 - val_loss: 0.7723\n",
      "Epoch 9/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.9189 - val_loss: 0.7518\n",
      "Epoch 10/50\n",
      "515/515 [==============================] - 0s 72us/step - loss: 0.8788 - val_loss: 0.7325\n",
      "Epoch 11/50\n",
      "515/515 [==============================] - 0s 70us/step - loss: 0.8421 - val_loss: 0.7141\n",
      "Epoch 12/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.8086 - val_loss: 0.6964\n",
      "Epoch 13/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.7779 - val_loss: 0.6793\n",
      "Epoch 14/50\n",
      "515/515 [==============================] - 0s 70us/step - loss: 0.7496 - val_loss: 0.6624\n",
      "Epoch 15/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.7228 - val_loss: 0.6455\n",
      "Epoch 16/50\n",
      "515/515 [==============================] - 0s 66us/step - loss: 0.6974 - val_loss: 0.6285\n",
      "Epoch 17/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.6729 - val_loss: 0.6112\n",
      "Epoch 18/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.6491 - val_loss: 0.5934\n",
      "Epoch 19/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.6258 - val_loss: 0.5751\n",
      "Epoch 20/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.6027 - val_loss: 0.5563\n",
      "Epoch 21/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.5797 - val_loss: 0.5372\n",
      "Epoch 22/50\n",
      "515/515 [==============================] - 0s 68us/step - loss: 0.5569 - val_loss: 0.5178\n",
      "Epoch 23/50\n",
      "515/515 [==============================] - 0s 66us/step - loss: 0.5343 - val_loss: 0.4984\n",
      "Epoch 24/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.5121 - val_loss: 0.4791\n",
      "Epoch 25/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.4905 - val_loss: 0.4602\n",
      "Epoch 26/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.4696 - val_loss: 0.4420\n",
      "Epoch 27/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.4497 - val_loss: 0.4245\n",
      "Epoch 28/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.4308 - val_loss: 0.4080\n",
      "Epoch 29/50\n",
      "515/515 [==============================] - 0s 70us/step - loss: 0.4132 - val_loss: 0.3928\n",
      "Epoch 30/50\n",
      "515/515 [==============================] - 0s 60us/step - loss: 0.3970 - val_loss: 0.3787\n",
      "Epoch 31/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.3823 - val_loss: 0.3661\n",
      "Epoch 32/50\n",
      "515/515 [==============================] - 0s 60us/step - loss: 0.3690 - val_loss: 0.3548\n",
      "Epoch 33/50\n",
      "515/515 [==============================] - 0s 66us/step - loss: 0.3573 - val_loss: 0.3446\n",
      "Epoch 34/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.3467 - val_loss: 0.3358\n",
      "Epoch 35/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.3376 - val_loss: 0.3279\n",
      "Epoch 36/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.3295 - val_loss: 0.3209\n",
      "Epoch 37/50\n",
      "515/515 [==============================] - 0s 70us/step - loss: 0.3223 - val_loss: 0.3150\n",
      "Epoch 38/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.3163 - val_loss: 0.3098\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 0s 66us/step - loss: 0.3110 - val_loss: 0.3052\n",
      "Epoch 40/50\n",
      "515/515 [==============================] - 0s 60us/step - loss: 0.3063 - val_loss: 0.3013\n",
      "Epoch 41/50\n",
      "515/515 [==============================] - 0s 66us/step - loss: 0.3024 - val_loss: 0.2979\n",
      "Epoch 42/50\n",
      "515/515 [==============================] - 0s 64us/step - loss: 0.2990 - val_loss: 0.2951\n",
      "Epoch 43/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.2961 - val_loss: 0.2925\n",
      "Epoch 44/50\n",
      "515/515 [==============================] - 0s 62us/step - loss: 0.2934 - val_loss: 0.2902\n",
      "Epoch 45/50\n",
      "515/515 [==============================] - 0s 60us/step - loss: 0.2911 - val_loss: 0.2883\n",
      "Epoch 46/50\n",
      "515/515 [==============================] - 0s 81us/step - loss: 0.2892 - val_loss: 0.2866\n",
      "Epoch 47/50\n",
      "515/515 [==============================] - 0s 127us/step - loss: 0.2875 - val_loss: 0.2851\n",
      "Epoch 48/50\n",
      "515/515 [==============================] - 0s 83us/step - loss: 0.2860 - val_loss: 0.2838\n",
      "Epoch 49/50\n",
      "515/515 [==============================] - 0s 60us/step - loss: 0.2847 - val_loss: 0.2826\n",
      "Epoch 50/50\n",
      "515/515 [==============================] - 0s 60us/step - loss: 0.2835 - val_loss: 0.2816\n",
      "The Accuracy for SVM model is %.2f 1.0\n",
      "Train on 399 samples, validate on 171 samples\n",
      "Epoch 1/50\n",
      "399/399 [==============================] - 4s 9ms/step - loss: 1.3820 - val_loss: 0.9197\n",
      "Epoch 2/50\n",
      "399/399 [==============================] - 0s 67us/step - loss: 1.2273 - val_loss: 0.8741\n",
      "Epoch 3/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 1.1227 - val_loss: 0.8412\n",
      "Epoch 4/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 1.0475 - val_loss: 0.8159\n",
      "Epoch 5/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.9899 - val_loss: 0.7955\n",
      "Epoch 6/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 0.9434 - val_loss: 0.7783\n",
      "Epoch 7/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 0.9046 - val_loss: 0.7637\n",
      "Epoch 8/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.8720 - val_loss: 0.7508\n",
      "Epoch 9/50\n",
      "399/399 [==============================] - 0s 57us/step - loss: 0.8440 - val_loss: 0.7393\n",
      "Epoch 10/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.8196 - val_loss: 0.7291\n",
      "Epoch 11/50\n",
      "399/399 [==============================] - 0s 57us/step - loss: 0.7982 - val_loss: 0.7201\n",
      "Epoch 12/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.7797 - val_loss: 0.7120\n",
      "Epoch 13/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 0.7636 - val_loss: 0.7047\n",
      "Epoch 14/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.7494 - val_loss: 0.6977\n",
      "Epoch 15/50\n",
      "399/399 [==============================] - 0s 57us/step - loss: 0.7361 - val_loss: 0.6910\n",
      "Epoch 16/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.7236 - val_loss: 0.6846\n",
      "Epoch 17/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.7120 - val_loss: 0.6785\n",
      "Epoch 18/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.7013 - val_loss: 0.6727\n",
      "Epoch 19/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.6915 - val_loss: 0.6672\n",
      "Epoch 20/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.6826 - val_loss: 0.6619\n",
      "Epoch 21/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 0.6741 - val_loss: 0.6567\n",
      "Epoch 22/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.6663 - val_loss: 0.6518\n",
      "Epoch 23/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.6591 - val_loss: 0.6470\n",
      "Epoch 24/50\n",
      "399/399 [==============================] - 0s 53us/step - loss: 0.6526 - val_loss: 0.6422\n",
      "Epoch 25/50\n",
      "399/399 [==============================] - 0s 53us/step - loss: 0.6464 - val_loss: 0.6375\n",
      "Epoch 26/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.6405 - val_loss: 0.6327\n",
      "Epoch 27/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 0.6348 - val_loss: 0.6279\n",
      "Epoch 28/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.6293 - val_loss: 0.6231\n",
      "Epoch 29/50\n",
      "399/399 [==============================] - 0s 60us/step - loss: 0.6240 - val_loss: 0.6181\n",
      "Epoch 30/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 0.6187 - val_loss: 0.6130\n",
      "Epoch 31/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.6134 - val_loss: 0.6077\n",
      "Epoch 32/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.6079 - val_loss: 0.6021\n",
      "Epoch 33/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.6022 - val_loss: 0.5963\n",
      "Epoch 34/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.5964 - val_loss: 0.5903\n",
      "Epoch 35/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.5903 - val_loss: 0.5840\n",
      "Epoch 36/50\n",
      "399/399 [==============================] - 0s 60us/step - loss: 0.5840 - val_loss: 0.5775\n",
      "Epoch 37/50\n",
      "399/399 [==============================] - 0s 57us/step - loss: 0.5775 - val_loss: 0.5707\n",
      "Epoch 38/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.5707 - val_loss: 0.5637\n",
      "Epoch 39/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.5636 - val_loss: 0.5564\n",
      "Epoch 40/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.5563 - val_loss: 0.5489\n",
      "Epoch 41/50\n",
      "399/399 [==============================] - 0s 65us/step - loss: 0.5488 - val_loss: 0.5412\n",
      "Epoch 42/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.5411 - val_loss: 0.5333\n",
      "Epoch 43/50\n",
      "399/399 [==============================] - 0s 53us/step - loss: 0.5332 - val_loss: 0.5252\n",
      "Epoch 44/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.5251 - val_loss: 0.5169\n",
      "Epoch 45/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.5168 - val_loss: 0.5085\n",
      "Epoch 46/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.5084 - val_loss: 0.5000\n",
      "Epoch 47/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.4999 - val_loss: 0.4915\n",
      "Epoch 48/50\n",
      "399/399 [==============================] - 0s 52us/step - loss: 0.4913 - val_loss: 0.4828\n",
      "Epoch 49/50\n",
      "399/399 [==============================] - 0s 55us/step - loss: 0.4827 - val_loss: 0.4742\n",
      "Epoch 50/50\n",
      "399/399 [==============================] - 0s 50us/step - loss: 0.4741 - val_loss: 0.4656\n",
      "The Accuracy for SVM model is %.2f 0.9005847953216374\n",
      "Train on 516 samples, validate on 222 samples\n",
      "Epoch 1/50\n",
      "516/516 [==============================] - 4s 7ms/step - loss: 1.6090 - val_loss: 1.0055\n",
      "Epoch 2/50\n",
      "516/516 [==============================] - 0s 62us/step - loss: 1.4154 - val_loss: 0.9444\n",
      "Epoch 3/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 1.2807 - val_loss: 0.9003\n",
      "Epoch 4/50\n",
      "516/516 [==============================] - 0s 70us/step - loss: 1.1832 - val_loss: 0.8655\n",
      "Epoch 5/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 1.1065 - val_loss: 0.8363\n",
      "Epoch 6/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 1.0426 - val_loss: 0.8108\n",
      "Epoch 7/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.9881 - val_loss: 0.7880\n",
      "Epoch 8/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.9405 - val_loss: 0.7670\n",
      "Epoch 9/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.8980 - val_loss: 0.7476\n",
      "Epoch 10/50\n",
      "516/516 [==============================] - 0s 70us/step - loss: 0.8596 - val_loss: 0.7293\n",
      "Epoch 11/50\n",
      "516/516 [==============================] - 0s 72us/step - loss: 0.8247 - val_loss: 0.7120\n",
      "Epoch 12/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.7926 - val_loss: 0.6955\n",
      "Epoch 13/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.7632 - val_loss: 0.6795\n",
      "Epoch 14/50\n",
      "516/516 [==============================] - 0s 65us/step - loss: 0.7358 - val_loss: 0.6637\n",
      "Epoch 15/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.7106 - val_loss: 0.6482\n",
      "Epoch 16/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.6867 - val_loss: 0.6328\n",
      "Epoch 17/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.6647 - val_loss: 0.6175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "516/516 [==============================] - 0s 70us/step - loss: 0.6440 - val_loss: 0.6021\n",
      "Epoch 19/50\n",
      "516/516 [==============================] - 0s 72us/step - loss: 0.6242 - val_loss: 0.5862\n",
      "Epoch 20/50\n",
      "516/516 [==============================] - 0s 69us/step - loss: 0.6047 - val_loss: 0.5697\n",
      "Epoch 21/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.5854 - val_loss: 0.5528\n",
      "Epoch 22/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.5661 - val_loss: 0.5353\n",
      "Epoch 23/50\n",
      "516/516 [==============================] - 0s 70us/step - loss: 0.5467 - val_loss: 0.5175\n",
      "Epoch 24/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.5272 - val_loss: 0.4994\n",
      "Epoch 25/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.5077 - val_loss: 0.4813\n",
      "Epoch 26/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.4884 - val_loss: 0.4634\n",
      "Epoch 27/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.4696 - val_loss: 0.4460\n",
      "Epoch 28/50\n",
      "516/516 [==============================] - 0s 72us/step - loss: 0.4515 - val_loss: 0.4292\n",
      "Epoch 29/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.4342 - val_loss: 0.4132\n",
      "Epoch 30/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.4176 - val_loss: 0.3981\n",
      "Epoch 31/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.4022 - val_loss: 0.3841\n",
      "Epoch 32/50\n",
      "516/516 [==============================] - 0s 70us/step - loss: 0.3879 - val_loss: 0.3713\n",
      "Epoch 33/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.3748 - val_loss: 0.3597\n",
      "Epoch 34/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.3629 - val_loss: 0.3493\n",
      "Epoch 35/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.3522 - val_loss: 0.3400\n",
      "Epoch 36/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.3426 - val_loss: 0.3317\n",
      "Epoch 37/50\n",
      "516/516 [==============================] - 0s 62us/step - loss: 0.3341 - val_loss: 0.3244\n",
      "Epoch 38/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.3267 - val_loss: 0.3181\n",
      "Epoch 39/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.3202 - val_loss: 0.3126\n",
      "Epoch 40/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.3145 - val_loss: 0.3077\n",
      "Epoch 41/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.3096 - val_loss: 0.3035\n",
      "Epoch 42/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.3053 - val_loss: 0.2999\n",
      "Epoch 43/50\n",
      "516/516 [==============================] - 0s 64us/step - loss: 0.3017 - val_loss: 0.2968\n",
      "Epoch 44/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.2985 - val_loss: 0.2940\n",
      "Epoch 45/50\n",
      "516/516 [==============================] - 0s 70us/step - loss: 0.2957 - val_loss: 0.2916\n",
      "Epoch 46/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.2932 - val_loss: 0.2895\n",
      "Epoch 47/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.2911 - val_loss: 0.2876\n",
      "Epoch 48/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.2892 - val_loss: 0.2859\n",
      "Epoch 49/50\n",
      "516/516 [==============================] - 0s 68us/step - loss: 0.2875 - val_loss: 0.2845\n",
      "Epoch 50/50\n",
      "516/516 [==============================] - 0s 66us/step - loss: 0.2860 - val_loss: 0.2832\n",
      "The Accuracy for SVM model is %.2f 0.30180180180180183\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "accruacyList = []\n",
    "dict_of_companies = {k: v for k, v in dfWithClustersID.groupby('labels')}\n",
    "for k in dict_of_companies:\n",
    "    Features, target = preprocessing(dict_of_companies[k], 'Text', 'Bi:Topics')\n",
    "    Features = Features.toarray()\n",
    "    \n",
    "    if(len(Features)>250):\n",
    "        X_train, X_test, y_train, y_test = splitDataset(Features, target, 0.3, 10)\n",
    "    \n",
    "        encoding_dim = 512 \n",
    "\n",
    "        input_img = Input(shape=(len(Features[0]),))\n",
    "\n",
    "        encoded = Dense(encoding_dim, activation='relu',activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "        encoded = Dense(256, activation='relu',activity_regularizer=regularizers.l1(10e-5))(encoded)\n",
    "        decoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "        decoded = Dense(len(Features[0]), activation='sigmoid')(decoded)\n",
    "\n",
    "            \n",
    "        autoencoder = Model(input_img, decoded)\n",
    "        encoder = Model(input_img, encoded)\n",
    "        encoded_input = Input(shape=(encoding_dim,))\n",
    "        decoder_layer = autoencoder.layers[-1]\n",
    "        \n",
    "        # decoder model\n",
    "        decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "        autoencoder.fit(X_train, X_train, \n",
    "                    epochs=50,\n",
    "                    batch_size=512,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test))\n",
    "        encoded_texts = encoder.predict(Features)\n",
    "\n",
    "        predicted, y_test = generateClassifier(encoded_texts, target, SVC)\n",
    "        accuracyScoreSVC = evaluateModel(y_test, predicted)\n",
    "        accruacyList.append(accuracyScoreSVC)\n",
    "        print(\"The Accuracy for SVM model is {0:.2f}\", accuracyScoreSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction and classification using MLP\n",
    "\n",
    "For me, MLP and SVM both work well. MLP is a powerful algorithm undoubtedly and it could give better results if we find the perfect hyperparameters. But as Nueral network is a blackbox algorithm, it is very hard to tune the hyperparameters perfectly. I have played with lots of hyperparameter to get the better result and this the best result i got so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1344 samples, validate on 577 samples\n",
      "Epoch 1/50\n",
      "1344/1344 [==============================] - 1s 1ms/step - loss: 0.7139 - val_loss: 0.7043\n",
      "Epoch 2/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.7018 - val_loss: 0.6944\n",
      "Epoch 3/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.6926 - val_loss: 0.6867\n",
      "Epoch 4/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.6852 - val_loss: 0.6800\n",
      "Epoch 5/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.6787 - val_loss: 0.6738\n",
      "Epoch 6/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.6725 - val_loss: 0.6677\n",
      "Epoch 7/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.6664 - val_loss: 0.6615\n",
      "Epoch 8/50\n",
      "1344/1344 [==============================] - 0s 16us/step - loss: 0.6602 - val_loss: 0.6551\n",
      "Epoch 9/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.6536 - val_loss: 0.6481\n",
      "Epoch 10/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.6465 - val_loss: 0.6405\n",
      "Epoch 11/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.6386 - val_loss: 0.6319\n",
      "Epoch 12/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.6299 - val_loss: 0.6224\n",
      "Epoch 13/50\n",
      "1344/1344 [==============================] - 0s 16us/step - loss: 0.6200 - val_loss: 0.6116\n",
      "Epoch 14/50\n",
      "1344/1344 [==============================] - 0s 16us/step - loss: 0.6090 - val_loss: 0.5996\n",
      "Epoch 15/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.5966 - val_loss: 0.5863\n",
      "Epoch 16/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.5830 - val_loss: 0.5716\n",
      "Epoch 17/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.5681 - val_loss: 0.5559\n",
      "Epoch 18/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.5522 - val_loss: 0.5392\n",
      "Epoch 19/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.5355 - val_loss: 0.5220\n",
      "Epoch 20/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.5183 - val_loss: 0.5046\n",
      "Epoch 21/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.5012 - val_loss: 0.4876\n",
      "Epoch 22/50\n",
      "1344/1344 [==============================] - 0s 12us/step - loss: 0.4845 - val_loss: 0.4713\n",
      "Epoch 23/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.4687 - val_loss: 0.4561\n",
      "Epoch 24/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.4541 - val_loss: 0.4422\n",
      "Epoch 25/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.4409 - val_loss: 0.4298\n",
      "Epoch 26/50\n",
      "1344/1344 [==============================] - 0s 16us/step - loss: 0.4291 - val_loss: 0.4189\n",
      "Epoch 27/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.4188 - val_loss: 0.4095\n",
      "Epoch 28/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.4099 - val_loss: 0.4014\n",
      "Epoch 29/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.4022 - val_loss: 0.3944\n",
      "Epoch 30/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.3956 - val_loss: 0.3884\n",
      "Epoch 31/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.3899 - val_loss: 0.3833\n",
      "Epoch 32/50\n",
      "1344/1344 [==============================] - 0s 16us/step - loss: 0.3851 - val_loss: 0.3788\n",
      "Epoch 33/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.3808 - val_loss: 0.3749\n",
      "Epoch 34/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.3771 - val_loss: 0.3715\n",
      "Epoch 35/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.3738 - val_loss: 0.3684\n",
      "Epoch 36/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.3708 - val_loss: 0.3657\n",
      "Epoch 37/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.3681 - val_loss: 0.3632\n",
      "Epoch 38/50\n",
      "1344/1344 [==============================] - 0s 11us/step - loss: 0.3657 - val_loss: 0.3611\n",
      "Epoch 39/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.3637 - val_loss: 0.3592\n",
      "Epoch 40/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.3619 - val_loss: 0.3576\n",
      "Epoch 41/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.3603 - val_loss: 0.3561\n",
      "Epoch 42/50\n",
      "1344/1344 [==============================] - 0s 17us/step - loss: 0.3588 - val_loss: 0.3546\n",
      "Epoch 43/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.3574 - val_loss: 0.3533\n",
      "Epoch 44/50\n",
      "1344/1344 [==============================] - 0s 15us/step - loss: 0.3561 - val_loss: 0.3521\n",
      "Epoch 45/50\n",
      "1344/1344 [==============================] - 0s 19us/step - loss: 0.3548 - val_loss: 0.3509\n",
      "Epoch 46/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.3536 - val_loss: 0.3498\n",
      "Epoch 47/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.3525 - val_loss: 0.3487\n",
      "Epoch 48/50\n",
      "1344/1344 [==============================] - 0s 14us/step - loss: 0.3514 - val_loss: 0.3476\n",
      "Epoch 49/50\n",
      "1344/1344 [==============================] - 0s 16us/step - loss: 0.3503 - val_loss: 0.3466\n",
      "Epoch 50/50\n",
      "1344/1344 [==============================] - 0s 13us/step - loss: 0.3493 - val_loss: 0.3456\n",
      "16\n",
      "Train on 1344 samples, validate on 577 samples\n",
      "Epoch 1/10\n",
      "1344/1344 [==============================] - 1s 1ms/step - loss: 3.4811 - acc: 0.2924 - val_loss: 1.8604 - val_acc: 0.4315\n",
      "Epoch 2/10\n",
      "1344/1344 [==============================] - 0s 92us/step - loss: 1.9912 - acc: 0.3311 - val_loss: 1.7527 - val_acc: 0.4315\n",
      "Epoch 3/10\n",
      "1344/1344 [==============================] - 0s 72us/step - loss: 1.8739 - acc: 0.3482 - val_loss: 1.6807 - val_acc: 0.4315\n",
      "Epoch 4/10\n",
      "1344/1344 [==============================] - 0s 70us/step - loss: 1.8261 - acc: 0.3609 - val_loss: 1.7072 - val_acc: 0.2652\n",
      "Epoch 5/10\n",
      "1344/1344 [==============================] - 0s 70us/step - loss: 1.8832 - acc: 0.2723 - val_loss: 1.7593 - val_acc: 0.4315\n",
      "Epoch 6/10\n",
      "1344/1344 [==============================] - 0s 70us/step - loss: 1.8277 - acc: 0.3400 - val_loss: 1.6556 - val_acc: 0.4315\n",
      "Epoch 7/10\n",
      "1344/1344 [==============================] - 0s 70us/step - loss: 1.8181 - acc: 0.3609 - val_loss: 1.7220 - val_acc: 0.2652\n",
      "Epoch 8/10\n",
      "1344/1344 [==============================] - 0s 70us/step - loss: 1.8158 - acc: 0.3371 - val_loss: 1.7185 - val_acc: 0.4315\n",
      "Epoch 9/10\n",
      "1344/1344 [==============================] - 0s 74us/step - loss: 1.8152 - acc: 0.3609 - val_loss: 1.6745 - val_acc: 0.4315\n",
      "Epoch 10/10\n",
      "1344/1344 [==============================] - 0s 67us/step - loss: 1.8025 - acc: 0.3609 - val_loss: 1.6879 - val_acc: 0.4315\n",
      "577/577 [==============================] - 0s 41us/step\n",
      "Test Loss Value: 1.687851933507407 Test Accuracy Score: 0.43154246100519933\n",
      "Train on 1948 samples, validate on 836 samples\n",
      "Epoch 1/50\n",
      "1948/1948 [==============================] - 1s 664us/step - loss: 0.7165 - val_loss: 0.7073\n",
      "Epoch 2/50\n",
      "1948/1948 [==============================] - 0s 12us/step - loss: 0.7023 - val_loss: 0.6950\n",
      "Epoch 3/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.6909 - val_loss: 0.6851\n",
      "Epoch 4/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6817 - val_loss: 0.6769\n",
      "Epoch 5/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6742 - val_loss: 0.6706\n",
      "Epoch 6/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6685 - val_loss: 0.6655\n",
      "Epoch 7/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6636 - val_loss: 0.6608\n",
      "Epoch 8/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.6589 - val_loss: 0.6561\n",
      "Epoch 9/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6542 - val_loss: 0.6514\n",
      "Epoch 10/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6495 - val_loss: 0.6466\n",
      "Epoch 11/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6446 - val_loss: 0.6415\n",
      "Epoch 12/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6395 - val_loss: 0.6361\n",
      "Epoch 13/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6337 - val_loss: 0.6297\n",
      "Epoch 14/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6268 - val_loss: 0.6218\n",
      "Epoch 15/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.6180 - val_loss: 0.6113\n",
      "Epoch 16/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.6061 - val_loss: 0.5968\n",
      "Epoch 17/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.5895 - val_loss: 0.5768\n",
      "Epoch 18/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.5670 - val_loss: 0.5502\n",
      "Epoch 19/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.5378 - val_loss: 0.5173\n",
      "Epoch 20/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.5033 - val_loss: 0.4809\n",
      "Epoch 21/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.4670 - val_loss: 0.4454\n",
      "Epoch 22/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.4336 - val_loss: 0.4150\n",
      "Epoch 23/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.4063 - val_loss: 0.3918\n",
      "Epoch 24/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3861 - val_loss: 0.3754\n",
      "Epoch 25/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3721 - val_loss: 0.3643\n",
      "Epoch 26/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3627 - val_loss: 0.3568\n",
      "Epoch 27/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3563 - val_loss: 0.3517\n",
      "Epoch 28/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3519 - val_loss: 0.3480\n",
      "Epoch 29/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3487 - val_loss: 0.3453\n",
      "Epoch 30/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3463 - val_loss: 0.3432\n",
      "Epoch 31/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3444 - val_loss: 0.3415\n",
      "Epoch 32/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.3428 - val_loss: 0.3401\n",
      "Epoch 33/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3415 - val_loss: 0.3389\n",
      "Epoch 34/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3403 - val_loss: 0.3377\n",
      "Epoch 35/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3392 - val_loss: 0.3367\n",
      "Epoch 36/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3382 - val_loss: 0.3358\n",
      "Epoch 37/50\n",
      "1948/1948 [==============================] - 0s 11us/step - loss: 0.3373 - val_loss: 0.3349\n",
      "Epoch 38/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3364 - val_loss: 0.3341\n",
      "Epoch 39/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3356 - val_loss: 0.3333\n",
      "Epoch 40/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3348 - val_loss: 0.3325\n",
      "Epoch 41/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3340 - val_loss: 0.3317\n",
      "Epoch 42/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3332 - val_loss: 0.3310\n",
      "Epoch 43/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3325 - val_loss: 0.3303\n",
      "Epoch 44/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3318 - val_loss: 0.3296\n",
      "Epoch 45/50\n",
      "1948/1948 [==============================] - 0s 8us/step - loss: 0.3311 - val_loss: 0.3290\n",
      "Epoch 46/50\n",
      "1948/1948 [==============================] - 0s 10us/step - loss: 0.3304 - val_loss: 0.3283\n",
      "Epoch 47/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3298 - val_loss: 0.3277\n",
      "Epoch 48/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3291 - val_loss: 0.3271\n",
      "Epoch 49/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3285 - val_loss: 0.3265\n",
      "Epoch 50/50\n",
      "1948/1948 [==============================] - 0s 9us/step - loss: 0.3279 - val_loss: 0.3259\n",
      "15\n",
      "Train on 1948 samples, validate on 836 samples\n",
      "Epoch 1/10\n",
      "1948/1948 [==============================] - 2s 784us/step - loss: 2.9782 - acc: 0.6391 - val_loss: 1.3648 - val_acc: 0.7380\n",
      "Epoch 2/10\n",
      "1948/1948 [==============================] - 0s 74us/step - loss: 1.2190 - acc: 0.7397 - val_loss: 1.2341 - val_acc: 0.7380\n",
      "Epoch 3/10\n",
      "1948/1948 [==============================] - 0s 66us/step - loss: 1.1062 - acc: 0.7397 - val_loss: 1.0440 - val_acc: 0.7380\n",
      "Epoch 4/10\n",
      "1948/1948 [==============================] - 0s 75us/step - loss: 1.0804 - acc: 0.7397 - val_loss: 1.0347 - val_acc: 0.7380\n",
      "Epoch 5/10\n",
      "1948/1948 [==============================] - 0s 61us/step - loss: 1.0575 - acc: 0.7397 - val_loss: 1.0432 - val_acc: 0.7380\n",
      "Epoch 6/10\n",
      "1948/1948 [==============================] - 0s 72us/step - loss: 1.0588 - acc: 0.7397 - val_loss: 1.0504 - val_acc: 0.7380\n",
      "Epoch 7/10\n",
      "1948/1948 [==============================] - 0s 64us/step - loss: 1.0683 - acc: 0.7397 - val_loss: 1.0701 - val_acc: 0.7380\n",
      "Epoch 8/10\n",
      "1948/1948 [==============================] - 0s 68us/step - loss: 1.0637 - acc: 0.7397 - val_loss: 1.0453 - val_acc: 0.7380\n",
      "Epoch 9/10\n",
      "1948/1948 [==============================] - 0s 66us/step - loss: 1.0505 - acc: 0.7397 - val_loss: 1.0301 - val_acc: 0.7380\n",
      "Epoch 10/10\n",
      "1948/1948 [==============================] - 0s 65us/step - loss: 1.0759 - acc: 0.7397 - val_loss: 1.0373 - val_acc: 0.7380\n",
      "836/836 [==============================] - 0s 37us/step\n",
      "Test Loss Value: 1.0373456153002651 Test Accuracy Score: 0.7380382775119617\n",
      "Train on 14416 samples, validate on 6179 samples\n",
      "Epoch 1/50\n",
      "14416/14416 [==============================] - 1s 102us/step - loss: 0.6766 - val_loss: 0.6018\n",
      "Epoch 2/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.4664 - val_loss: 0.3772\n",
      "Epoch 3/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3636 - val_loss: 0.3554\n",
      "Epoch 4/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3508 - val_loss: 0.3467\n",
      "Epoch 5/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3434 - val_loss: 0.3402\n",
      "Epoch 6/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3375 - val_loss: 0.3349\n",
      "Epoch 7/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.3326 - val_loss: 0.3304\n",
      "Epoch 8/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3285 - val_loss: 0.3266\n",
      "Epoch 9/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.3249 - val_loss: 0.3233\n",
      "Epoch 10/50\n",
      "14416/14416 [==============================] - 0s 9us/step - loss: 0.3218 - val_loss: 0.3204\n",
      "Epoch 11/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.3191 - val_loss: 0.3179\n",
      "Epoch 12/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3167 - val_loss: 0.3156\n",
      "Epoch 13/50\n",
      "14416/14416 [==============================] - 0s 9us/step - loss: 0.3145 - val_loss: 0.3135\n",
      "Epoch 14/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3125 - val_loss: 0.3116\n",
      "Epoch 15/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3107 - val_loss: 0.3099\n",
      "Epoch 16/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3091 - val_loss: 0.3084\n",
      "Epoch 17/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3076 - val_loss: 0.3069\n",
      "Epoch 18/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3062 - val_loss: 0.3056\n",
      "Epoch 19/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3049 - val_loss: 0.3044\n",
      "Epoch 20/50\n",
      "14416/14416 [==============================] - 0s 9us/step - loss: 0.3038 - val_loss: 0.3033\n",
      "Epoch 21/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3027 - val_loss: 0.3022\n",
      "Epoch 22/50\n",
      "14416/14416 [==============================] - 0s 9us/step - loss: 0.3016 - val_loss: 0.3012\n",
      "Epoch 23/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.3007 - val_loss: 0.3003\n",
      "Epoch 24/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2998 - val_loss: 0.2995\n",
      "Epoch 25/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2990 - val_loss: 0.2986\n",
      "Epoch 26/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2982 - val_loss: 0.2979\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2974 - val_loss: 0.2972\n",
      "Epoch 28/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2967 - val_loss: 0.2965\n",
      "Epoch 29/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2960 - val_loss: 0.2958\n",
      "Epoch 30/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.2954 - val_loss: 0.2952\n",
      "Epoch 31/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2948 - val_loss: 0.2946\n",
      "Epoch 32/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2942 - val_loss: 0.2940\n",
      "Epoch 33/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.2936 - val_loss: 0.2935\n",
      "Epoch 34/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2931 - val_loss: 0.2929\n",
      "Epoch 35/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.2926 - val_loss: 0.2924\n",
      "Epoch 36/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2921 - val_loss: 0.2919\n",
      "Epoch 37/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2916 - val_loss: 0.2914\n",
      "Epoch 38/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2911 - val_loss: 0.2910\n",
      "Epoch 39/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.2907 - val_loss: 0.2905\n",
      "Epoch 40/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2902 - val_loss: 0.2901\n",
      "Epoch 41/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2898 - val_loss: 0.2897\n",
      "Epoch 42/50\n",
      "14416/14416 [==============================] - 0s 9us/step - loss: 0.2894 - val_loss: 0.2893\n",
      "Epoch 43/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.2890 - val_loss: 0.2889\n",
      "Epoch 44/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2886 - val_loss: 0.2885\n",
      "Epoch 45/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2882 - val_loss: 0.2881\n",
      "Epoch 46/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2878 - val_loss: 0.2878\n",
      "Epoch 47/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2875 - val_loss: 0.2874\n",
      "Epoch 48/50\n",
      "14416/14416 [==============================] - 0s 9us/step - loss: 0.2871 - val_loss: 0.2871\n",
      "Epoch 49/50\n",
      "14416/14416 [==============================] - 0s 8us/step - loss: 0.2868 - val_loss: 0.2867\n",
      "Epoch 50/50\n",
      "14416/14416 [==============================] - 0s 7us/step - loss: 0.2864 - val_loss: 0.2864\n",
      "16\n",
      "Train on 14416 samples, validate on 6179 samples\n",
      "Epoch 1/10\n",
      "14416/14416 [==============================] - 3s 204us/step - loss: 3.0001 - acc: 0.1824 - val_loss: 2.8461 - val_acc: 0.1937\n",
      "Epoch 2/10\n",
      "14416/14416 [==============================] - 1s 69us/step - loss: 2.8202 - acc: 0.1922 - val_loss: 2.8417 - val_acc: 0.1756\n",
      "Epoch 3/10\n",
      "14416/14416 [==============================] - 1s 60us/step - loss: 2.8177 - acc: 0.1892 - val_loss: 2.8801 - val_acc: 0.1937\n",
      "Epoch 4/10\n",
      "14416/14416 [==============================] - 1s 64us/step - loss: 2.8176 - acc: 0.1885 - val_loss: 2.8540 - val_acc: 0.1937\n",
      "Epoch 5/10\n",
      "14416/14416 [==============================] - 1s 72us/step - loss: 2.8143 - acc: 0.1892 - val_loss: 2.8309 - val_acc: 0.1937\n",
      "Epoch 6/10\n",
      "14416/14416 [==============================] - 1s 72us/step - loss: 2.8101 - acc: 0.1926 - val_loss: 2.8273 - val_acc: 0.1937\n",
      "Epoch 7/10\n",
      "14416/14416 [==============================] - 1s 73us/step - loss: 2.8092 - acc: 0.1930 - val_loss: 2.8729 - val_acc: 0.1937\n",
      "Epoch 8/10\n",
      "14416/14416 [==============================] - 1s 75us/step - loss: 2.8083 - acc: 0.1939 - val_loss: 2.8440 - val_acc: 0.1756\n",
      "Epoch 9/10\n",
      "14416/14416 [==============================] - 1s 73us/step - loss: 2.8039 - acc: 0.1946 - val_loss: 2.8342 - val_acc: 0.1756\n",
      "Epoch 10/10\n",
      "14416/14416 [==============================] - 1s 72us/step - loss: 2.8017 - acc: 0.1891 - val_loss: 2.8085 - val_acc: 0.1939\n",
      "6179/6179 [==============================] - 0s 48us/step\n",
      "Test Loss Value: 2.8085482541093705 Test Accuracy Score: 0.19388250526457393\n",
      "Train on 705 samples, validate on 303 samples\n",
      "Epoch 1/50\n",
      "705/705 [==============================] - 1s 2ms/step - loss: 0.7559 - val_loss: 0.7459\n",
      "Epoch 2/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.7464 - val_loss: 0.7371\n",
      "Epoch 3/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.7377 - val_loss: 0.7290\n",
      "Epoch 4/50\n",
      "705/705 [==============================] - 0s 14us/step - loss: 0.7296 - val_loss: 0.7215\n",
      "Epoch 5/50\n",
      "705/705 [==============================] - 0s 11us/step - loss: 0.7221 - val_loss: 0.7145\n",
      "Epoch 6/50\n",
      "705/705 [==============================] - 0s 14us/step - loss: 0.7151 - val_loss: 0.7080\n",
      "Epoch 7/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.7086 - val_loss: 0.7019\n",
      "Epoch 8/50\n",
      "705/705 [==============================] - 0s 20us/step - loss: 0.7026 - val_loss: 0.6963\n",
      "Epoch 9/50\n",
      "705/705 [==============================] - 0s 17us/step - loss: 0.6970 - val_loss: 0.6910\n",
      "Epoch 10/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6916 - val_loss: 0.6858\n",
      "Epoch 11/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6865 - val_loss: 0.6808\n",
      "Epoch 12/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6814 - val_loss: 0.6758\n",
      "Epoch 13/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6763 - val_loss: 0.6706\n",
      "Epoch 14/50\n",
      "705/705 [==============================] - 0s 11us/step - loss: 0.6711 - val_loss: 0.6651\n",
      "Epoch 15/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6655 - val_loss: 0.6593\n",
      "Epoch 16/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.6596 - val_loss: 0.6530\n",
      "Epoch 17/50\n",
      "705/705 [==============================] - 0s 11us/step - loss: 0.6532 - val_loss: 0.6461\n",
      "Epoch 18/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6461 - val_loss: 0.6385\n",
      "Epoch 19/50\n",
      "705/705 [==============================] - 0s 11us/step - loss: 0.6383 - val_loss: 0.6301\n",
      "Epoch 20/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6297 - val_loss: 0.6207\n",
      "Epoch 21/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.6201 - val_loss: 0.6103\n",
      "Epoch 22/50\n",
      "705/705 [==============================] - 0s 11us/step - loss: 0.6095 - val_loss: 0.5988\n",
      "Epoch 23/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.5978 - val_loss: 0.5862\n",
      "Epoch 24/50\n",
      "705/705 [==============================] - 0s 11us/step - loss: 0.5850 - val_loss: 0.5726\n",
      "Epoch 25/50\n",
      "705/705 [==============================] - 0s 12us/step - loss: 0.5712 - val_loss: 0.5580\n",
      "Epoch 26/50\n",
      "705/705 [==============================] - 0s 14us/step - loss: 0.5565 - val_loss: 0.5427\n",
      "Epoch 27/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.5410 - val_loss: 0.5267\n",
      "Epoch 28/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.5251 - val_loss: 0.5104\n",
      "Epoch 29/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.5089 - val_loss: 0.4941\n",
      "Epoch 30/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.4928 - val_loss: 0.4781\n",
      "Epoch 31/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.4771 - val_loss: 0.4627\n",
      "Epoch 32/50\n",
      "705/705 [==============================] - 0s 11us/step - loss: 0.4620 - val_loss: 0.4481\n",
      "Epoch 33/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.4478 - val_loss: 0.4345\n",
      "Epoch 34/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.4347 - val_loss: 0.4221\n",
      "Epoch 35/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.4227 - val_loss: 0.4109\n",
      "Epoch 36/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.4119 - val_loss: 0.4008\n",
      "Epoch 37/50\n",
      "705/705 [==============================] - 0s 14us/step - loss: 0.4023 - val_loss: 0.3919\n",
      "Epoch 38/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.3938 - val_loss: 0.3841\n",
      "Epoch 39/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.3863 - val_loss: 0.3773\n",
      "Epoch 40/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.3798 - val_loss: 0.3713\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705/705 [==============================] - 0s 13us/step - loss: 0.3741 - val_loss: 0.3661\n",
      "Epoch 42/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.3692 - val_loss: 0.3616\n",
      "Epoch 43/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.3648 - val_loss: 0.3576\n",
      "Epoch 44/50\n",
      "705/705 [==============================] - 0s 14us/step - loss: 0.3610 - val_loss: 0.3541\n",
      "Epoch 45/50\n",
      "705/705 [==============================] - 0s 14us/step - loss: 0.3577 - val_loss: 0.3511\n",
      "Epoch 46/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.3548 - val_loss: 0.3485\n",
      "Epoch 47/50\n",
      "705/705 [==============================] - 0s 16us/step - loss: 0.3523 - val_loss: 0.3461\n",
      "Epoch 48/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.3500 - val_loss: 0.3440\n",
      "Epoch 49/50\n",
      "705/705 [==============================] - 0s 13us/step - loss: 0.3479 - val_loss: 0.3421\n",
      "Epoch 50/50\n",
      "705/705 [==============================] - 0s 14us/step - loss: 0.3461 - val_loss: 0.3404\n",
      "16\n",
      "Train on 705 samples, validate on 303 samples\n",
      "Epoch 1/10\n",
      "705/705 [==============================] - 2s 2ms/step - loss: 4.1295 - acc: 0.6355 - val_loss: 2.6448 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "705/705 [==============================] - 0s 92us/step - loss: 1.6019 - acc: 0.9986 - val_loss: 0.0581 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "705/705 [==============================] - 0s 71us/step - loss: 0.0466 - acc: 0.9986 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "705/705 [==============================] - 0s 69us/step - loss: 0.0210 - acc: 0.9986 - val_loss: 4.5884e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "705/705 [==============================] - 0s 71us/step - loss: 0.0223 - acc: 0.9986 - val_loss: 7.5439e-05 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "705/705 [==============================] - 0s 71us/step - loss: 0.0229 - acc: 0.9986 - val_loss: 1.2314e-05 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "705/705 [==============================] - 0s 71us/step - loss: 0.0229 - acc: 0.9986 - val_loss: 2.5827e-06 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "705/705 [==============================] - 0s 74us/step - loss: 0.0229 - acc: 0.9986 - val_loss: 5.8070e-07 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "705/705 [==============================] - 0s 81us/step - loss: 0.0229 - acc: 0.9986 - val_loss: 1.8078e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "705/705 [==============================] - 0s 75us/step - loss: 0.0229 - acc: 0.9986 - val_loss: 1.2688e-07 - val_acc: 1.0000\n",
      "303/303 [==============================] - 0s 49us/step\n",
      "Test Loss Value: 1.2688117466257266e-07 Test Accuracy Score: 1.0\n",
      "Train on 8226 samples, validate on 3526 samples\n",
      "Epoch 1/50\n",
      "8226/8226 [==============================] - 2s 189us/step - loss: 0.7024 - val_loss: 0.6791\n",
      "Epoch 2/50\n",
      "8226/8226 [==============================] - 0s 10us/step - loss: 0.6632 - val_loss: 0.6440\n",
      "Epoch 3/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.6217 - val_loss: 0.5895\n",
      "Epoch 4/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.5505 - val_loss: 0.5024\n",
      "Epoch 5/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.4658 - val_loss: 0.4303\n",
      "Epoch 6/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.4111 - val_loss: 0.3924\n",
      "Epoch 7/50\n",
      "8226/8226 [==============================] - 0s 7us/step - loss: 0.3821 - val_loss: 0.3715\n",
      "Epoch 8/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.3653 - val_loss: 0.3585\n",
      "Epoch 9/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.3547 - val_loss: 0.3495\n",
      "Epoch 10/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3462 - val_loss: 0.3419\n",
      "Epoch 11/50\n",
      "8226/8226 [==============================] - 0s 7us/step - loss: 0.3407 - val_loss: 0.3384\n",
      "Epoch 12/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3375 - val_loss: 0.3355\n",
      "Epoch 13/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3348 - val_loss: 0.3330\n",
      "Epoch 14/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.3323 - val_loss: 0.3306\n",
      "Epoch 15/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3300 - val_loss: 0.3284\n",
      "Epoch 16/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3278 - val_loss: 0.3263\n",
      "Epoch 17/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3258 - val_loss: 0.3244\n",
      "Epoch 18/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3239 - val_loss: 0.3226\n",
      "Epoch 19/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3221 - val_loss: 0.3209\n",
      "Epoch 20/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3205 - val_loss: 0.3193\n",
      "Epoch 21/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.3189 - val_loss: 0.3178\n",
      "Epoch 22/50\n",
      "8226/8226 [==============================] - 0s 8us/step - loss: 0.3175 - val_loss: 0.3164\n",
      "Epoch 23/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3161 - val_loss: 0.3151\n",
      "Epoch 24/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3148 - val_loss: 0.3138\n",
      "Epoch 25/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3136 - val_loss: 0.3126\n",
      "Epoch 26/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3124 - val_loss: 0.3115\n",
      "Epoch 27/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3113 - val_loss: 0.3104\n",
      "Epoch 28/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3102 - val_loss: 0.3094\n",
      "Epoch 29/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3093 - val_loss: 0.3085\n",
      "Epoch 30/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3083 - val_loss: 0.3076\n",
      "Epoch 31/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3074 - val_loss: 0.3067\n",
      "Epoch 32/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3066 - val_loss: 0.3059\n",
      "Epoch 33/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3057 - val_loss: 0.3051\n",
      "Epoch 34/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3050 - val_loss: 0.3043\n",
      "Epoch 35/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3042 - val_loss: 0.3036\n",
      "Epoch 36/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3035 - val_loss: 0.3029\n",
      "Epoch 37/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3028 - val_loss: 0.3022\n",
      "Epoch 38/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3022 - val_loss: 0.3016\n",
      "Epoch 39/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3015 - val_loss: 0.3010\n",
      "Epoch 40/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3009 - val_loss: 0.3004\n",
      "Epoch 41/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.3003 - val_loss: 0.2999\n",
      "Epoch 42/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.2998 - val_loss: 0.2993\n",
      "Epoch 43/50\n",
      "8226/8226 [==============================] - 0s 9us/step - loss: 0.2993 - val_loss: 0.2988\n",
      "Epoch 44/50\n",
      "8226/8226 [==============================] - 0s 12us/step - loss: 0.2987 - val_loss: 0.2983\n",
      "Epoch 45/50\n",
      "8226/8226 [==============================] - 0s 13us/step - loss: 0.2982 - val_loss: 0.2978\n",
      "Epoch 46/50\n",
      "8226/8226 [==============================] - 0s 11us/step - loss: 0.2978 - val_loss: 0.2973\n",
      "Epoch 47/50\n",
      "8226/8226 [==============================] - 0s 11us/step - loss: 0.2973 - val_loss: 0.2969\n",
      "Epoch 48/50\n",
      "8226/8226 [==============================] - 0s 11us/step - loss: 0.2968 - val_loss: 0.2964\n",
      "Epoch 49/50\n",
      "8226/8226 [==============================] - 0s 12us/step - loss: 0.2964 - val_loss: 0.2960\n",
      "Epoch 50/50\n",
      "8226/8226 [==============================] - 0s 12us/step - loss: 0.2960 - val_loss: 0.2956\n",
      "16\n",
      "Train on 8226 samples, validate on 3526 samples\n",
      "Epoch 1/10\n",
      "8226/8226 [==============================] - 2s 274us/step - loss: 2.6717 - acc: 0.4604 - val_loss: 2.3617 - val_acc: 0.4671\n",
      "Epoch 2/10\n",
      "8226/8226 [==============================] - 1s 71us/step - loss: 2.2941 - acc: 0.4731 - val_loss: 2.3407 - val_acc: 0.4671\n",
      "Epoch 3/10\n",
      "8226/8226 [==============================] - 1s 71us/step - loss: 2.2991 - acc: 0.4731 - val_loss: 2.7906 - val_acc: 0.4671\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8226/8226 [==============================] - 1s 71us/step - loss: 2.2974 - acc: 0.4731 - val_loss: 2.3317 - val_acc: 0.4671\n",
      "Epoch 5/10\n",
      "8226/8226 [==============================] - 1s 71us/step - loss: 2.2836 - acc: 0.4731 - val_loss: 2.3560 - val_acc: 0.4671\n",
      "Epoch 6/10\n",
      "8226/8226 [==============================] - 1s 73us/step - loss: 2.2800 - acc: 0.4731 - val_loss: 2.3312 - val_acc: 0.4671\n",
      "Epoch 7/10\n",
      "8226/8226 [==============================] - 1s 70us/step - loss: 2.2787 - acc: 0.4731 - val_loss: 2.3261 - val_acc: 0.4671\n",
      "Epoch 8/10\n",
      "8226/8226 [==============================] - 1s 71us/step - loss: 2.2759 - acc: 0.4731 - val_loss: 2.3746 - val_acc: 0.4671\n",
      "Epoch 9/10\n",
      "8226/8226 [==============================] - 1s 71us/step - loss: 2.2833 - acc: 0.4731 - val_loss: 2.3994 - val_acc: 0.4671\n",
      "Epoch 10/10\n",
      "8226/8226 [==============================] - 1s 69us/step - loss: 2.2808 - acc: 0.4731 - val_loss: 2.3477 - val_acc: 0.4671\n",
      "3526/3526 [==============================] - 0s 51us/step\n",
      "Test Loss Value: 2.3476659075076514 Test Accuracy Score: 0.4671015314804311\n",
      "Train on 4301 samples, validate on 1844 samples\n",
      "Epoch 1/50\n",
      "4301/4301 [==============================] - 2s 373us/step - loss: 0.7104 - val_loss: 0.6924\n",
      "Epoch 2/50\n",
      "4301/4301 [==============================] - 0s 12us/step - loss: 0.6795 - val_loss: 0.6640\n",
      "Epoch 3/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.6484 - val_loss: 0.6280\n",
      "Epoch 4/50\n",
      "4301/4301 [==============================] - 0s 8us/step - loss: 0.6044 - val_loss: 0.5724\n",
      "Epoch 5/50\n",
      "4301/4301 [==============================] - 0s 8us/step - loss: 0.5390 - val_loss: 0.4982\n",
      "Epoch 6/50\n",
      "4301/4301 [==============================] - 0s 8us/step - loss: 0.4654 - val_loss: 0.4304\n",
      "Epoch 7/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.4092 - val_loss: 0.3883\n",
      "Epoch 8/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3778 - val_loss: 0.3670\n",
      "Epoch 9/50\n",
      "4301/4301 [==============================] - 0s 13us/step - loss: 0.3622 - val_loss: 0.3564\n",
      "Epoch 10/50\n",
      "4301/4301 [==============================] - 0s 12us/step - loss: 0.3542 - val_loss: 0.3506\n",
      "Epoch 11/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3495 - val_loss: 0.3468\n",
      "Epoch 12/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3463 - val_loss: 0.3441\n",
      "Epoch 13/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3438 - val_loss: 0.3419\n",
      "Epoch 14/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3418 - val_loss: 0.3400\n",
      "Epoch 15/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3399 - val_loss: 0.3382\n",
      "Epoch 16/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3382 - val_loss: 0.3365\n",
      "Epoch 17/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3365 - val_loss: 0.3350\n",
      "Epoch 18/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3350 - val_loss: 0.3334\n",
      "Epoch 19/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3335 - val_loss: 0.3320\n",
      "Epoch 20/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3320 - val_loss: 0.3306\n",
      "Epoch 21/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3307 - val_loss: 0.3293\n",
      "Epoch 22/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3293 - val_loss: 0.3280\n",
      "Epoch 23/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3280 - val_loss: 0.3267\n",
      "Epoch 24/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3268 - val_loss: 0.3255\n",
      "Epoch 25/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3256 - val_loss: 0.3244\n",
      "Epoch 26/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3245 - val_loss: 0.3233\n",
      "Epoch 27/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3234 - val_loss: 0.3222\n",
      "Epoch 28/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3223 - val_loss: 0.3212\n",
      "Epoch 29/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3213 - val_loss: 0.3202\n",
      "Epoch 30/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3203 - val_loss: 0.3192\n",
      "Epoch 31/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3193 - val_loss: 0.3183\n",
      "Epoch 32/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3184 - val_loss: 0.3174\n",
      "Epoch 33/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3175 - val_loss: 0.3165\n",
      "Epoch 34/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3167 - val_loss: 0.3157\n",
      "Epoch 35/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3158 - val_loss: 0.3149\n",
      "Epoch 36/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3150 - val_loss: 0.3141\n",
      "Epoch 37/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3142 - val_loss: 0.3134\n",
      "Epoch 38/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3135 - val_loss: 0.3126\n",
      "Epoch 39/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3128 - val_loss: 0.3119\n",
      "Epoch 40/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3120 - val_loss: 0.3112\n",
      "Epoch 41/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3114 - val_loss: 0.3106\n",
      "Epoch 42/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3107 - val_loss: 0.3099\n",
      "Epoch 43/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3101 - val_loss: 0.3093\n",
      "Epoch 44/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3094 - val_loss: 0.3087\n",
      "Epoch 45/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3088 - val_loss: 0.3081\n",
      "Epoch 46/50\n",
      "4301/4301 [==============================] - 0s 12us/step - loss: 0.3082 - val_loss: 0.3075\n",
      "Epoch 47/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3077 - val_loss: 0.3070\n",
      "Epoch 48/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3071 - val_loss: 0.3064\n",
      "Epoch 49/50\n",
      "4301/4301 [==============================] - 0s 9us/step - loss: 0.3066 - val_loss: 0.3059\n",
      "Epoch 50/50\n",
      "4301/4301 [==============================] - 0s 10us/step - loss: 0.3060 - val_loss: 0.3054\n",
      "16\n",
      "Train on 4301 samples, validate on 1844 samples\n",
      "Epoch 1/10\n",
      "4301/4301 [==============================] - 2s 469us/step - loss: 2.9677 - acc: 0.2237 - val_loss: 2.4020 - val_acc: 0.2663\n",
      "Epoch 2/10\n",
      "4301/4301 [==============================] - 0s 69us/step - loss: 2.4033 - acc: 0.2481 - val_loss: 2.3719 - val_acc: 0.2663\n",
      "Epoch 3/10\n",
      "4301/4301 [==============================] - 0s 71us/step - loss: 2.3860 - acc: 0.2692 - val_loss: 2.3715 - val_acc: 0.2663\n",
      "Epoch 4/10\n",
      "4301/4301 [==============================] - 0s 73us/step - loss: 2.3789 - acc: 0.2630 - val_loss: 2.3704 - val_acc: 0.2505\n",
      "Epoch 5/10\n",
      "4301/4301 [==============================] - 0s 72us/step - loss: 2.3779 - acc: 0.2637 - val_loss: 2.3601 - val_acc: 0.2663\n",
      "Epoch 6/10\n",
      "4301/4301 [==============================] - 0s 71us/step - loss: 2.3758 - acc: 0.2551 - val_loss: 2.3721 - val_acc: 0.2663\n",
      "Epoch 7/10\n",
      "4301/4301 [==============================] - 0s 70us/step - loss: 2.3782 - acc: 0.2706 - val_loss: 2.3618 - val_acc: 0.2663\n",
      "Epoch 8/10\n",
      "4301/4301 [==============================] - 0s 77us/step - loss: 2.3684 - acc: 0.2788 - val_loss: 2.3797 - val_acc: 0.2663\n",
      "Epoch 9/10\n",
      "4301/4301 [==============================] - 0s 71us/step - loss: 2.3801 - acc: 0.2723 - val_loss: 2.3626 - val_acc: 0.2663\n",
      "Epoch 10/10\n",
      "4301/4301 [==============================] - 0s 68us/step - loss: 2.3672 - acc: 0.2683 - val_loss: 2.3783 - val_acc: 0.2663\n",
      "1844/1844 [==============================] - 0s 54us/step\n",
      "Test Loss Value: 2.3782924757603707 Test Accuracy Score: 0.2662689804125763\n",
      "Train on 931 samples, validate on 400 samples\n",
      "Epoch 1/50\n",
      "931/931 [==============================] - 2s 2ms/step - loss: 0.7295 - val_loss: 0.7201\n",
      "Epoch 2/50\n",
      "931/931 [==============================] - 0s 30us/step - loss: 0.7198 - val_loss: 0.7116\n",
      "Epoch 3/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.7114 - val_loss: 0.7042\n",
      "Epoch 4/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.7040 - val_loss: 0.6975\n",
      "Epoch 5/50\n",
      "931/931 [==============================] - 0s 12us/step - loss: 0.6974 - val_loss: 0.6914\n",
      "Epoch 6/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.6914 - val_loss: 0.6859\n",
      "Epoch 7/50\n",
      "931/931 [==============================] - 0s 12us/step - loss: 0.6858 - val_loss: 0.6805\n",
      "Epoch 8/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.6804 - val_loss: 0.6752\n",
      "Epoch 9/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.6751 - val_loss: 0.6699\n",
      "Epoch 10/50\n",
      "931/931 [==============================] - 0s 11us/step - loss: 0.6699 - val_loss: 0.6646\n",
      "Epoch 11/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.6645 - val_loss: 0.6590\n",
      "Epoch 12/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.6588 - val_loss: 0.6529\n",
      "Epoch 13/50\n",
      "931/931 [==============================] - 0s 11us/step - loss: 0.6526 - val_loss: 0.6461\n",
      "Epoch 14/50\n",
      "931/931 [==============================] - 0s 12us/step - loss: 0.6456 - val_loss: 0.6385\n",
      "Epoch 15/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.6379 - val_loss: 0.6300\n",
      "Epoch 16/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.6292 - val_loss: 0.6203\n",
      "Epoch 17/50\n",
      "931/931 [==============================] - 0s 10us/step - loss: 0.6193 - val_loss: 0.6095\n",
      "Epoch 18/50\n",
      "931/931 [==============================] - 0s 12us/step - loss: 0.6082 - val_loss: 0.5973\n",
      "Epoch 19/50\n",
      "931/931 [==============================] - 0s 12us/step - loss: 0.5958 - val_loss: 0.5838\n",
      "Epoch 20/50\n",
      "931/931 [==============================] - 0s 11us/step - loss: 0.5821 - val_loss: 0.5690\n",
      "Epoch 21/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.5671 - val_loss: 0.5531\n",
      "Epoch 22/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.5511 - val_loss: 0.5362\n",
      "Epoch 23/50\n",
      "931/931 [==============================] - 0s 12us/step - loss: 0.5343 - val_loss: 0.5188\n",
      "Epoch 24/50\n",
      "931/931 [==============================] - 0s 11us/step - loss: 0.5169 - val_loss: 0.5010\n",
      "Epoch 25/50\n",
      "931/931 [==============================] - 0s 12us/step - loss: 0.4995 - val_loss: 0.4835\n",
      "Epoch 26/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.4824 - val_loss: 0.4665\n",
      "Epoch 27/50\n",
      "931/931 [==============================] - 0s 11us/step - loss: 0.4659 - val_loss: 0.4504\n",
      "Epoch 28/50\n",
      "931/931 [==============================] - 0s 19us/step - loss: 0.4504 - val_loss: 0.4355\n",
      "Epoch 29/50\n",
      "931/931 [==============================] - 0s 20us/step - loss: 0.4362 - val_loss: 0.4220\n",
      "Epoch 30/50\n",
      "931/931 [==============================] - 0s 17us/step - loss: 0.4233 - val_loss: 0.4098\n",
      "Epoch 31/50\n",
      "931/931 [==============================] - 0s 19us/step - loss: 0.4118 - val_loss: 0.3991\n",
      "Epoch 32/50\n",
      "931/931 [==============================] - 0s 17us/step - loss: 0.4017 - val_loss: 0.3897\n",
      "Epoch 33/50\n",
      "931/931 [==============================] - 0s 16us/step - loss: 0.3929 - val_loss: 0.3815\n",
      "Epoch 34/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.3852 - val_loss: 0.3744\n",
      "Epoch 35/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.3785 - val_loss: 0.3682\n",
      "Epoch 36/50\n",
      "931/931 [==============================] - ETA: 0s - loss: 0.379 - 0s 12us/step - loss: 0.3727 - val_loss: 0.3629\n",
      "Epoch 37/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.3676 - val_loss: 0.3582\n",
      "Epoch 38/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.3632 - val_loss: 0.3541\n",
      "Epoch 39/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.3594 - val_loss: 0.3507\n",
      "Epoch 40/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.3563 - val_loss: 0.3478\n",
      "Epoch 41/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.3536 - val_loss: 0.3454\n",
      "Epoch 42/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.3513 - val_loss: 0.3433\n",
      "Epoch 43/50\n",
      "931/931 [==============================] - 0s 15us/step - loss: 0.3493 - val_loss: 0.3414\n",
      "Epoch 44/50\n",
      "931/931 [==============================] - 0s 15us/step - loss: 0.3476 - val_loss: 0.3398\n",
      "Epoch 45/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.3460 - val_loss: 0.3383\n",
      "Epoch 46/50\n",
      "931/931 [==============================] - ETA: 0s - loss: 0.350 - 0s 15us/step - loss: 0.3447 - val_loss: 0.3370\n",
      "Epoch 47/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.3434 - val_loss: 0.3358\n",
      "Epoch 48/50\n",
      "931/931 [==============================] - 0s 13us/step - loss: 0.3423 - val_loss: 0.3348\n",
      "Epoch 49/50\n",
      "931/931 [==============================] - 0s 14us/step - loss: 0.3413 - val_loss: 0.3338\n",
      "Epoch 50/50\n",
      "931/931 [==============================] - 0s 15us/step - loss: 0.3403 - val_loss: 0.3329\n",
      "16\n",
      "Train on 931 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "931/931 [==============================] - 2s 2ms/step - loss: 3.5332 - acc: 0.6788 - val_loss: 0.8252 - val_acc: 0.9225\n",
      "Epoch 2/10\n",
      "931/931 [==============================] - 0s 73us/step - loss: 0.4939 - acc: 0.9452 - val_loss: 0.5405 - val_acc: 0.9225\n",
      "Epoch 3/10\n",
      "931/931 [==============================] - 0s 77us/step - loss: 0.3766 - acc: 0.9452 - val_loss: 0.4861 - val_acc: 0.9225\n",
      "Epoch 4/10\n",
      "931/931 [==============================] - 0s 66us/step - loss: 0.3463 - acc: 0.9452 - val_loss: 0.4445 - val_acc: 0.9225\n",
      "Epoch 5/10\n",
      "931/931 [==============================] - 0s 68us/step - loss: 0.3298 - acc: 0.9452 - val_loss: 0.4564 - val_acc: 0.9225\n",
      "Epoch 6/10\n",
      "931/931 [==============================] - 0s 71us/step - loss: 0.3214 - acc: 0.9452 - val_loss: 0.4381 - val_acc: 0.9225\n",
      "Epoch 7/10\n",
      "931/931 [==============================] - 0s 58us/step - loss: 0.3148 - acc: 0.9452 - val_loss: 0.4619 - val_acc: 0.9225\n",
      "Epoch 8/10\n",
      "931/931 [==============================] - 0s 95us/step - loss: 0.3182 - acc: 0.9452 - val_loss: 0.4640 - val_acc: 0.9225\n",
      "Epoch 9/10\n",
      "931/931 [==============================] - 0s 73us/step - loss: 0.3151 - acc: 0.9452 - val_loss: 0.4598 - val_acc: 0.9225\n",
      "Epoch 10/10\n",
      "931/931 [==============================] - 0s 76us/step - loss: 0.3114 - acc: 0.9452 - val_loss: 0.4380 - val_acc: 0.9225\n",
      "400/400 [==============================] - 0s 61us/step\n",
      "Test Loss Value: 0.4380162519216537 Test Accuracy Score: 0.9225\n",
      "Train on 515 samples, validate on 221 samples\n",
      "Epoch 1/50\n",
      "515/515 [==============================] - 2s 3ms/step - loss: 0.7332 - val_loss: 0.7227\n",
      "Epoch 2/50\n",
      "515/515 [==============================] - 0s 36us/step - loss: 0.7272 - val_loss: 0.7171\n",
      "Epoch 3/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.7214 - val_loss: 0.7118\n",
      "Epoch 4/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.7160 - val_loss: 0.7069\n",
      "Epoch 5/50\n",
      "515/515 [==============================] - 0s 16us/step - loss: 0.7110 - val_loss: 0.7021\n",
      "Epoch 6/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.7062 - val_loss: 0.6976\n",
      "Epoch 7/50\n",
      "515/515 [==============================] - 0s 18us/step - loss: 0.7017 - val_loss: 0.6933\n",
      "Epoch 8/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6973 - val_loss: 0.6891\n",
      "Epoch 9/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.6931 - val_loss: 0.6850\n",
      "Epoch 10/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.6890 - val_loss: 0.6809\n",
      "Epoch 11/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6848 - val_loss: 0.6767\n",
      "Epoch 12/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6807 - val_loss: 0.6726\n",
      "Epoch 13/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6766 - val_loss: 0.6684\n",
      "Epoch 14/50\n",
      "515/515 [==============================] - 0s 16us/step - loss: 0.6725 - val_loss: 0.6641\n",
      "Epoch 15/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6683 - val_loss: 0.6597\n",
      "Epoch 16/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6639 - val_loss: 0.6550\n",
      "Epoch 17/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6593 - val_loss: 0.6502\n",
      "Epoch 18/50\n",
      "515/515 [==============================] - 0s 21us/step - loss: 0.6546 - val_loss: 0.6451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.6496 - val_loss: 0.6398\n",
      "Epoch 20/50\n",
      "515/515 [==============================] - 0s 19us/step - loss: 0.6443 - val_loss: 0.6341\n",
      "Epoch 21/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.6388 - val_loss: 0.6281\n",
      "Epoch 22/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6329 - val_loss: 0.6217\n",
      "Epoch 23/50\n",
      "515/515 [==============================] - 0s 16us/step - loss: 0.6267 - val_loss: 0.6150\n",
      "Epoch 24/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6201 - val_loss: 0.6079\n",
      "Epoch 25/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.6132 - val_loss: 0.6004\n",
      "Epoch 26/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.6059 - val_loss: 0.5926\n",
      "Epoch 27/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.5982 - val_loss: 0.5843\n",
      "Epoch 28/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.5901 - val_loss: 0.5757\n",
      "Epoch 29/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.5818 - val_loss: 0.5669\n",
      "Epoch 30/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.5731 - val_loss: 0.5578\n",
      "Epoch 31/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.5643 - val_loss: 0.5484\n",
      "Epoch 32/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.5552 - val_loss: 0.5390\n",
      "Epoch 33/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.5460 - val_loss: 0.5293\n",
      "Epoch 34/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.5366 - val_loss: 0.5198\n",
      "Epoch 35/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.5274 - val_loss: 0.5104\n",
      "Epoch 36/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.5183 - val_loss: 0.5010\n",
      "Epoch 37/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.5092 - val_loss: 0.4918\n",
      "Epoch 38/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.5003 - val_loss: 0.4830\n",
      "Epoch 39/50\n",
      "515/515 [==============================] - 0s 14us/step - loss: 0.4918 - val_loss: 0.4744\n",
      "Epoch 40/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.4836 - val_loss: 0.4663\n",
      "Epoch 41/50\n",
      "515/515 [==============================] - 0s 16us/step - loss: 0.4757 - val_loss: 0.4585\n",
      "Epoch 42/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.4683 - val_loss: 0.4512\n",
      "Epoch 43/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.4612 - val_loss: 0.4444\n",
      "Epoch 44/50\n",
      "515/515 [==============================] - 0s 19us/step - loss: 0.4547 - val_loss: 0.4380\n",
      "Epoch 45/50\n",
      "515/515 [==============================] - 0s 21us/step - loss: 0.4486 - val_loss: 0.4321\n",
      "Epoch 46/50\n",
      "515/515 [==============================] - 0s 16us/step - loss: 0.4429 - val_loss: 0.4266\n",
      "Epoch 47/50\n",
      "515/515 [==============================] - 0s 19us/step - loss: 0.4376 - val_loss: 0.4215\n",
      "Epoch 48/50\n",
      "515/515 [==============================] - 0s 13us/step - loss: 0.4328 - val_loss: 0.4169\n",
      "Epoch 49/50\n",
      "515/515 [==============================] - 0s 17us/step - loss: 0.4284 - val_loss: 0.4126\n",
      "Epoch 50/50\n",
      "515/515 [==============================] - 0s 15us/step - loss: 0.4242 - val_loss: 0.4086\n",
      "15\n",
      "Train on 515 samples, validate on 221 samples\n",
      "Epoch 1/10\n",
      "515/515 [==============================] - 2s 4ms/step - loss: 4.4007 - acc: 0.4990 - val_loss: 2.0253 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "515/515 [==============================] - 0s 83us/step - loss: 1.2904 - acc: 0.9961 - val_loss: 0.0184 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "515/515 [==============================] - 0s 103us/step - loss: 0.0409 - acc: 0.9961 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "515/515 [==============================] - 0s 81us/step - loss: 0.0369 - acc: 0.9961 - val_loss: 2.4119e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "515/515 [==============================] - 0s 85us/step - loss: 0.0387 - acc: 0.9961 - val_loss: 7.3322e-05 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "515/515 [==============================] - 0s 87us/step - loss: 0.0407 - acc: 0.9961 - val_loss: 4.5875e-05 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "515/515 [==============================] - 0s 85us/step - loss: 0.0387 - acc: 0.9961 - val_loss: 6.4868e-05 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "515/515 [==============================] - 0s 91us/step - loss: 0.0367 - acc: 0.9961 - val_loss: 1.5138e-04 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "515/515 [==============================] - 0s 91us/step - loss: 0.0319 - acc: 0.9961 - val_loss: 5.1478e-04 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "515/515 [==============================] - 0s 91us/step - loss: 0.0277 - acc: 0.9961 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "221/221 [==============================] - 0s 72us/step\n",
      "Test Loss Value: 0.0014914054106457404 Test Accuracy Score: 1.0\n",
      "Train on 399 samples, validate on 171 samples\n",
      "Epoch 1/50\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 0.7179 - val_loss: 0.7096\n",
      "Epoch 2/50\n",
      "399/399 [==============================] - 0s 35us/step - loss: 0.7136 - val_loss: 0.7059\n",
      "Epoch 3/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.7097 - val_loss: 0.7026\n",
      "Epoch 4/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.7062 - val_loss: 0.6996\n",
      "Epoch 5/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.7030 - val_loss: 0.6969\n",
      "Epoch 6/50\n",
      "399/399 [==============================] - 0s 18us/step - loss: 0.7001 - val_loss: 0.6945\n",
      "Epoch 7/50\n",
      "399/399 [==============================] - 0s 12us/step - loss: 0.6975 - val_loss: 0.6922\n",
      "Epoch 8/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.6952 - val_loss: 0.6901\n",
      "Epoch 9/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6929 - val_loss: 0.6880\n",
      "Epoch 10/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6908 - val_loss: 0.6861\n",
      "Epoch 11/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6887 - val_loss: 0.6842\n",
      "Epoch 12/50\n",
      "399/399 [==============================] - 0s 18us/step - loss: 0.6868 - val_loss: 0.6824\n",
      "Epoch 13/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6849 - val_loss: 0.6807\n",
      "Epoch 14/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6831 - val_loss: 0.6790\n",
      "Epoch 15/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6813 - val_loss: 0.6773\n",
      "Epoch 16/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6796 - val_loss: 0.6757\n",
      "Epoch 17/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6780 - val_loss: 0.6741\n",
      "Epoch 18/50\n",
      "399/399 [==============================] - 0s 12us/step - loss: 0.6763 - val_loss: 0.6725\n",
      "Epoch 19/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6747 - val_loss: 0.6710\n",
      "Epoch 20/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6732 - val_loss: 0.6695\n",
      "Epoch 21/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6716 - val_loss: 0.6680\n",
      "Epoch 22/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.6701 - val_loss: 0.6665\n",
      "Epoch 23/50\n",
      "399/399 [==============================] - 0s 18us/step - loss: 0.6685 - val_loss: 0.6650\n",
      "Epoch 24/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6670 - val_loss: 0.6635\n",
      "Epoch 25/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6655 - val_loss: 0.6620\n",
      "Epoch 26/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.6640 - val_loss: 0.6605\n",
      "Epoch 27/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6626 - val_loss: 0.6590\n",
      "Epoch 28/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.6611 - val_loss: 0.6575\n",
      "Epoch 29/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6596 - val_loss: 0.6559\n",
      "Epoch 30/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6581 - val_loss: 0.6544\n",
      "Epoch 31/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6566 - val_loss: 0.6529\n",
      "Epoch 32/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6550 - val_loss: 0.6513\n",
      "Epoch 33/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6535 - val_loss: 0.6497\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399/399 [==============================] - 0s 17us/step - loss: 0.6520 - val_loss: 0.6481\n",
      "Epoch 35/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6504 - val_loss: 0.6465\n",
      "Epoch 36/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6488 - val_loss: 0.6448\n",
      "Epoch 37/50\n",
      "399/399 [==============================] - 0s 18us/step - loss: 0.6471 - val_loss: 0.6431\n",
      "Epoch 38/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6455 - val_loss: 0.6413\n",
      "Epoch 39/50\n",
      "399/399 [==============================] - 0s 23us/step - loss: 0.6438 - val_loss: 0.6395\n",
      "Epoch 40/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6420 - val_loss: 0.6377\n",
      "Epoch 41/50\n",
      "399/399 [==============================] - 0s 18us/step - loss: 0.6403 - val_loss: 0.6358\n",
      "Epoch 42/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6385 - val_loss: 0.6338\n",
      "Epoch 43/50\n",
      "399/399 [==============================] - 0s 17us/step - loss: 0.6366 - val_loss: 0.6318\n",
      "Epoch 44/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6347 - val_loss: 0.6298\n",
      "Epoch 45/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.6327 - val_loss: 0.6276\n",
      "Epoch 46/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6307 - val_loss: 0.6254\n",
      "Epoch 47/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.6286 - val_loss: 0.6232\n",
      "Epoch 48/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6264 - val_loss: 0.6208\n",
      "Epoch 49/50\n",
      "399/399 [==============================] - 0s 20us/step - loss: 0.6241 - val_loss: 0.6184\n",
      "Epoch 50/50\n",
      "399/399 [==============================] - 0s 15us/step - loss: 0.6218 - val_loss: 0.6158\n",
      "15\n",
      "Train on 399 samples, validate on 171 samples\n",
      "Epoch 1/10\n",
      "399/399 [==============================] - 2s 5ms/step - loss: 4.5889 - acc: 0.3358 - val_loss: 4.3032 - val_acc: 0.9006\n",
      "Epoch 2/10\n",
      "399/399 [==============================] - 0s 110us/step - loss: 4.1546 - acc: 0.9348 - val_loss: 3.2479 - val_acc: 0.9006\n",
      "Epoch 3/10\n",
      "399/399 [==============================] - 0s 110us/step - loss: 2.7911 - acc: 0.9348 - val_loss: 0.8935 - val_acc: 0.9006\n",
      "Epoch 4/10\n",
      "399/399 [==============================] - 0s 82us/step - loss: 0.6529 - acc: 0.9348 - val_loss: 0.6882 - val_acc: 0.9006\n",
      "Epoch 5/10\n",
      "399/399 [==============================] - 0s 87us/step - loss: 0.4783 - acc: 0.9348 - val_loss: 0.6772 - val_acc: 0.9006\n",
      "Epoch 6/10\n",
      "399/399 [==============================] - 0s 80us/step - loss: 0.4663 - acc: 0.9348 - val_loss: 0.6297 - val_acc: 0.9006\n",
      "Epoch 7/10\n",
      "399/399 [==============================] - 0s 82us/step - loss: 0.4374 - acc: 0.9348 - val_loss: 0.6108 - val_acc: 0.9006\n",
      "Epoch 8/10\n",
      "399/399 [==============================] - 0s 83us/step - loss: 0.4199 - acc: 0.9348 - val_loss: 0.5648 - val_acc: 0.9006\n",
      "Epoch 9/10\n",
      "399/399 [==============================] - 0s 87us/step - loss: 0.3981 - acc: 0.9348 - val_loss: 0.5492 - val_acc: 0.9006\n",
      "Epoch 10/10\n",
      "399/399 [==============================] - 0s 92us/step - loss: 0.3886 - acc: 0.9348 - val_loss: 0.5423 - val_acc: 0.9006\n",
      "171/171 [==============================] - 0s 87us/step\n",
      "Test Loss Value: 0.5422949567872878 Test Accuracy Score: 0.9005847949730722\n",
      "Train on 516 samples, validate on 222 samples\n",
      "Epoch 1/50\n",
      "516/516 [==============================] - 2s 4ms/step - loss: 0.7118 - val_loss: 0.7060\n",
      "Epoch 2/50\n",
      "516/516 [==============================] - 0s 21us/step - loss: 0.7075 - val_loss: 0.7023\n",
      "Epoch 3/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.7037 - val_loss: 0.6988\n",
      "Epoch 4/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.7001 - val_loss: 0.6956\n",
      "Epoch 5/50\n",
      "516/516 [==============================] - 0s 16us/step - loss: 0.6968 - val_loss: 0.6927\n",
      "Epoch 6/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.6938 - val_loss: 0.6898\n",
      "Epoch 7/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6909 - val_loss: 0.6872\n",
      "Epoch 8/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6881 - val_loss: 0.6846\n",
      "Epoch 9/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.6855 - val_loss: 0.6821\n",
      "Epoch 10/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6830 - val_loss: 0.6798\n",
      "Epoch 11/50\n",
      "516/516 [==============================] - 0s 14us/step - loss: 0.6806 - val_loss: 0.6775\n",
      "Epoch 12/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6783 - val_loss: 0.6753\n",
      "Epoch 13/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6761 - val_loss: 0.6732\n",
      "Epoch 14/50\n",
      "516/516 [==============================] - 0s 16us/step - loss: 0.6739 - val_loss: 0.6711\n",
      "Epoch 15/50\n",
      "516/516 [==============================] - 0s 16us/step - loss: 0.6718 - val_loss: 0.6690\n",
      "Epoch 16/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6698 - val_loss: 0.6670\n",
      "Epoch 17/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6677 - val_loss: 0.6650\n",
      "Epoch 18/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6658 - val_loss: 0.6630\n",
      "Epoch 19/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6638 - val_loss: 0.6610\n",
      "Epoch 20/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.6618 - val_loss: 0.6590\n",
      "Epoch 21/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6599 - val_loss: 0.6569\n",
      "Epoch 22/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.6579 - val_loss: 0.6549\n",
      "Epoch 23/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.6559 - val_loss: 0.6528\n",
      "Epoch 24/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6539 - val_loss: 0.6506\n",
      "Epoch 25/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.6518 - val_loss: 0.6484\n",
      "Epoch 26/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6497 - val_loss: 0.6461\n",
      "Epoch 27/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6475 - val_loss: 0.6437\n",
      "Epoch 28/50\n",
      "516/516 [==============================] - 0s 16us/step - loss: 0.6452 - val_loss: 0.6412\n",
      "Epoch 29/50\n",
      "516/516 [==============================] - 0s 21us/step - loss: 0.6428 - val_loss: 0.6384\n",
      "Epoch 30/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6402 - val_loss: 0.6355\n",
      "Epoch 31/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6374 - val_loss: 0.6324\n",
      "Epoch 32/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6344 - val_loss: 0.6289\n",
      "Epoch 33/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.6311 - val_loss: 0.6252\n",
      "Epoch 34/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6275 - val_loss: 0.6211\n",
      "Epoch 35/50\n",
      "516/516 [==============================] - 0s 16us/step - loss: 0.6236 - val_loss: 0.6167\n",
      "Epoch 36/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6194 - val_loss: 0.6118\n",
      "Epoch 37/50\n",
      "516/516 [==============================] - 0s 21us/step - loss: 0.6146 - val_loss: 0.6064\n",
      "Epoch 38/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.6094 - val_loss: 0.6004\n",
      "Epoch 39/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.6037 - val_loss: 0.5940\n",
      "Epoch 40/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.5975 - val_loss: 0.5870\n",
      "Epoch 41/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.5907 - val_loss: 0.5794\n",
      "Epoch 42/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.5834 - val_loss: 0.5712\n",
      "Epoch 43/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.5755 - val_loss: 0.5625\n",
      "Epoch 44/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.5671 - val_loss: 0.5532\n",
      "Epoch 45/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.5581 - val_loss: 0.5435\n",
      "Epoch 46/50\n",
      "516/516 [==============================] - 0s 17us/step - loss: 0.5488 - val_loss: 0.5335\n",
      "Epoch 47/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.5391 - val_loss: 0.5232\n",
      "Epoch 48/50\n",
      "516/516 [==============================] - 0s 15us/step - loss: 0.5292 - val_loss: 0.5128\n",
      "Epoch 49/50\n",
      "516/516 [==============================] - 0s 21us/step - loss: 0.5192 - val_loss: 0.5024\n",
      "Epoch 50/50\n",
      "516/516 [==============================] - 0s 19us/step - loss: 0.5092 - val_loss: 0.4922\n",
      "16\n",
      "Train on 516 samples, validate on 222 samples\n",
      "Epoch 1/10\n",
      "516/516 [==============================] - 2s 4ms/step - loss: 4.5867 - acc: 0.0969 - val_loss: 4.0912 - val_acc: 0.0856\n",
      "Epoch 2/10\n",
      "516/516 [==============================] - 0s 92us/step - loss: 3.8798 - acc: 0.1279 - val_loss: 3.3117 - val_acc: 0.1351\n",
      "Epoch 3/10\n",
      "516/516 [==============================] - 0s 86us/step - loss: 3.2097 - acc: 0.1725 - val_loss: 2.9128 - val_acc: 0.3018\n",
      "Epoch 4/10\n",
      "516/516 [==============================] - 0s 87us/step - loss: 3.0365 - acc: 0.2016 - val_loss: 2.9882 - val_acc: 0.3018\n",
      "Epoch 5/10\n",
      "516/516 [==============================] - 0s 81us/step - loss: 3.0437 - acc: 0.2016 - val_loss: 2.7701 - val_acc: 0.3018\n",
      "Epoch 6/10\n",
      "516/516 [==============================] - 0s 95us/step - loss: 2.9025 - acc: 0.2016 - val_loss: 2.9465 - val_acc: 0.1351\n",
      "Epoch 7/10\n",
      "516/516 [==============================] - 0s 93us/step - loss: 2.9032 - acc: 0.1686 - val_loss: 3.0452 - val_acc: 0.1351\n",
      "Epoch 8/10\n",
      "516/516 [==============================] - 0s 91us/step - loss: 2.9247 - acc: 0.1841 - val_loss: 2.9867 - val_acc: 0.3018\n",
      "Epoch 9/10\n",
      "516/516 [==============================] - 0s 91us/step - loss: 2.8721 - acc: 0.2016 - val_loss: 2.8329 - val_acc: 0.3018\n",
      "Epoch 10/10\n",
      "516/516 [==============================] - 0s 93us/step - loss: 2.8537 - acc: 0.2016 - val_loss: 2.7855 - val_acc: 0.3018\n",
      "222/222 [==============================] - 0s 72us/step\n",
      "Test Loss Value: 2.785482945742908 Test Accuracy Score: 0.30180180207029117\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import models, layers, optimizers, datasets, utils\n",
    "dict_of_companies = {k: v for k, v in dfWithClustersID.groupby('labels')}\n",
    "nnAccuracyList = []\n",
    "for k in dict_of_companies:\n",
    "    Features, target = preprocessing(dict_of_companies[k], 'Text', 'Bi:Topics')\n",
    "    Features = Features.toarray()\n",
    "    \n",
    "    if(len(Features)>250):\n",
    "        X_train, X_test, y_train, y_test = splitDataset(Features, target, 0.3, 10)\n",
    "    \n",
    "        encoding_dim = int(len(Features[0])/2) \n",
    "\n",
    "        input_img = Input(shape=(len(Features[0]),))\n",
    "\n",
    "        encoded = Dense(encoding_dim, activation='relu',activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "        decoded = Dense(len(Features[0]), activation='sigmoid')(encoded)\n",
    "\n",
    "        autoencoder = Model(input_img, decoded)\n",
    "        encoder = Model(input_img, encoded)\n",
    "        encoded_input = Input(shape=(encoding_dim,))\n",
    "        decoder_layer = autoencoder.layers[-1]\n",
    "        decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "        autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "        autoencoder.fit(X_train, X_train, \n",
    "                    epochs=50,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test))\n",
    "        encoded_texts = encoder.predict(Features)\n",
    "        \n",
    "        \n",
    "        X_train, X_test, y_train, y_test = splitDataset(encoded_texts, target, 0.3, 25)\n",
    "        y_train = utils.to_categorical(y_train, 103)\n",
    "        y_test = utils.to_categorical(y_test, 103)\n",
    "        input_dim = (len(encoded_texts[0]))\n",
    "        print(input_dim)\n",
    "        mlpInputs = layers.Input(shape=(input_dim,))\n",
    "        n = layers.Dense(512, activation='relu')(mlpInputs)\n",
    "        n = layers.Dense(512, activation='relu')(n)\n",
    "        n = layers.Dense(256, activation='relu')(n)\n",
    "        n = layers.Dense(128, activation='relu')(n)\n",
    "        outcomes= layers.Dense(103, activation='softmax')(n)\n",
    "\n",
    "        mlpModel = models.Model(inputs=mlpInputs, outputs=outcomes)\n",
    "\n",
    "        mlpModel.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "        history=mlpModel.fit(X_train, y_train, batch_size=256, epochs=10, validation_data=(X_test, y_test))\n",
    "        valScore = mlpModel.evaluate(X_test, y_test)\n",
    "        print('Test Loss Value:', valScore[0],'Test Accuracy Score:', valScore[1])\n",
    "        nnAccuracyList.append(valScore[1])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the accuracy score before and after feature extraction and using different algorithms\n",
    "\n",
    "After clustering and Before feature extraction the accuracy was more and after feature selection the accuracy got reduced for SVM. because the dimension of the data got recuced. Hence the accuracy will not have much difference before feature selection or after feature selection, but we will be able to run the classifier effectively which are expensive to train.\n",
    "\n",
    "Very Large cluster and very small cluster does not have good accuracy after doing feature slection using autoencoder.\n",
    "\n",
    "Reason: For small cluster, there is no enough data to train the model,hence giving low accuracy. For very large cluster,while reducing dimension, it may discard the important features or the data could be unbalanced too.\n",
    "\n",
    "after doing clustering I got really good accuracy using SVM algorithm because each cluster has almost similar documents. Hence, Applying classifier to each cluster yields the more accuracy.\n",
    " \n",
    "Clustering is semi-supervised learning, where first we identify the realtion between data(the falls under the same cluster). Meaning, even if the some of the data have different labels, it can fall under the same cluster. So that is unsupervised learning to get some knowledge about the relation in data. After that we apply lassification algorithm to clssify the data which is supervised approach.\n",
    "\n",
    "This learning helps to improve the accuracy and feature selection helps to reduce the dimensionality of he given data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\\n\n",
    "[1] https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html <br/>\n",
    "[2] https://blog.keras.io/building-autoencoders-in-keras.html <br/>\n",
    "[3] https://www.scikit-yb.org/en/latest/api/cluster/elbow.html <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
